{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc1d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 23:18:16,035 [INFO] \n",
      "======================================================================\n",
      "2025-12-17 23:18:16,036 [INFO] STARTING ENTITY STRUCTURING PIPELINE\n",
      "2025-12-17 23:18:16,036 [INFO] ======================================================================\n",
      "\n",
      "2025-12-17 23:18:16,038 [INFO] Configuration loaded from config.json\n",
      "2025-12-17 23:18:16,038 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,038 [INFO] CONFIGURATION LOADED\n",
      "2025-12-17 23:18:16,038 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,038 [INFO] Incremental Processing:\n",
      "2025-12-17 23:18:16,044 [INFO]   Enabled: True\n",
      "2025-12-17 23:18:16,045 [INFO]   State Directory: state\n",
      "2025-12-17 23:18:16,046 [INFO]   State File: processed_message_ids.txt\n",
      "2025-12-17 23:18:16,047 [INFO]   Checkpoint Interval: 50\n",
      "2025-12-17 23:18:16,048 [INFO]   Force Full Reprocess: False\n",
      "2025-12-17 23:18:16,049 [INFO] Input/Output:\n",
      "2025-12-17 23:18:16,050 [INFO]   Input File: ../Phase 2/relevant_placement_emails.csv\n",
      "2025-12-17 23:18:16,050 [INFO]   Output CSV: structured_job_postings.csv\n",
      "2025-12-17 23:18:16,051 [INFO]   Output JSON: structured_job_postings.json\n",
      "2025-12-17 23:18:16,052 [INFO] Processing:\n",
      "2025-12-17 23:18:16,053 [INFO]   Max Jobs Per Email: 5\n",
      "2025-12-17 23:18:16,055 [INFO]   Max Companies Per Email: 3\n",
      "2025-12-17 23:18:16,057 [INFO]   Max Positions Per Email: 3\n",
      "2025-12-17 23:18:16,057 [INFO]   Min Completeness Score: 0.3\n",
      "2025-12-17 23:18:16,060 [INFO]   Enable Analytics: True\n",
      "2025-12-17 23:18:16,061 [INFO] Logging:\n",
      "2025-12-17 23:18:16,063 [INFO]   Level: INFO\n",
      "2025-12-17 23:18:16,064 [INFO]   File: entity_structuring.log\n",
      "2025-12-17 23:18:16,065 [INFO]   Performance Metrics: True\n",
      "2025-12-17 23:18:16,067 [INFO] Normalization:\n",
      "2025-12-17 23:18:16,069 [INFO]   Skill mappings: 13\n",
      "2025-12-17 23:18:16,070 [INFO]   Degree mappings: 14\n",
      "2025-12-17 23:18:16,072 [INFO]   City mappings: 15\n",
      "2025-12-17 23:18:16,074 [INFO]   Company suffixes: 9\n",
      "2025-12-17 23:18:16,075 [INFO] Position Levels:\n",
      "2025-12-17 23:18:16,077 [INFO]   Senior keywords: 4\n",
      "2025-12-17 23:18:16,077 [INFO]   Junior keywords: 3\n",
      "2025-12-17 23:18:16,079 [INFO]   Intern keywords: 2\n",
      "2025-12-17 23:18:16,080 [INFO]   Manager keywords: 3\n",
      "2025-12-17 23:18:16,082 [INFO] Salary Parsing:\n",
      "2025-12-17 23:18:16,083 [INFO]   Patterns: 5\n",
      "2025-12-17 23:18:16,084 [INFO]   Default currency: INR\n",
      "2025-12-17 23:18:16,086 [INFO] Experience Parsing:\n",
      "2025-12-17 23:18:16,086 [INFO]   Patterns: 3\n",
      "2025-12-17 23:18:16,089 [INFO] Deadline Parsing:\n",
      "2025-12-17 23:18:16,090 [INFO]   Date patterns: 3\n",
      "2025-12-17 23:18:16,091 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,094 [INFO] No existing state file found, starting fresh\n",
      "2025-12-17 23:18:16,096 [INFO] ======================================================================\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 31-32: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 942, in <module>\n",
      "    jobs_df, jobs_json = main()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 917, in main\n",
      "    pipeline = EntityStructuringPipeline()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 656, in __init__\n",
      "    self.logger.info(\"üèóÔ∏è  ENTITY STRUCTURING PIPELINE INITIALIZED\")\n",
      "Message: 'üèóÔ∏è  ENTITY STRUCTURING PIPELINE INITIALIZED'\n",
      "Arguments: ()\n",
      "2025-12-17 23:18:16,096 [INFO] üèóÔ∏è  ENTITY STRUCTURING PIPELINE INITIALIZED\n",
      "2025-12-17 23:18:16,447 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,448 [INFO] \n",
      "Loading dataset: ../Phase 2/relevant_placement_emails.csv\n",
      "2025-12-17 23:18:16,592 [INFO] Loaded 233 emails\n",
      "2025-12-17 23:18:16,600 [INFO] Total emails: 233, Already processed: 0, New to process: 233\n",
      "2025-12-17 23:18:16,617 [INFO] Loaded 74 existing job postings\n",
      "2025-12-17 23:18:16,617 [INFO] \n",
      "Processing 233 new emails...\n",
      "2025-12-17 23:18:16,632 [INFO] Saved 50 processed message IDs to state\n",
      "2025-12-17 23:18:16,634 [INFO]    Checkpoint: 50/233 emails | Jobs extracted: 41\n",
      "2025-12-17 23:18:16,645 [INFO] Saved 100 processed message IDs to state\n",
      "2025-12-17 23:18:16,647 [INFO]    Checkpoint: 100/233 emails | Jobs extracted: 68\n",
      "2025-12-17 23:18:16,648 [INFO] Saved 150 processed message IDs to state\n",
      "2025-12-17 23:18:16,648 [INFO]    Checkpoint: 150/233 emails | Jobs extracted: 71\n",
      "2025-12-17 23:18:16,662 [INFO] Saved 200 processed message IDs to state\n",
      "2025-12-17 23:18:16,664 [INFO]    Checkpoint: 200/233 emails | Jobs extracted: 74\n",
      "2025-12-17 23:18:16,667 [INFO] Saved 233 processed message IDs to state\n",
      "2025-12-17 23:18:16,667 [INFO] \n",
      "======================================================================\n",
      "2025-12-17 23:18:16,667 [INFO] PROCESSING COMPLETE\n",
      "2025-12-17 23:18:16,667 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,667 [INFO] New Emails Processed: 233\n",
      "2025-12-17 23:18:16,667 [INFO] New Emails with Jobs: 26\n",
      "2025-12-17 23:18:16,675 [INFO] New Job Postings: 74\n",
      "2025-12-17 23:18:16,676 [INFO] Total Job Postings: 148\n",
      "2025-12-17 23:18:16,678 [INFO] ======================================================================\n",
      "\n",
      "2025-12-17 23:18:16,680 [INFO] Saving JSON: structured_job_postings.json\n",
      "2025-12-17 23:18:16,707 [INFO] Saved 148 jobs to JSON\n",
      "2025-12-17 23:18:16,711 [INFO] Saving CSV: structured_job_postings.csv\n",
      "2025-12-17 23:18:16,730 [INFO] Saved 148 jobs to CSV\n",
      "\n",
      "2025-12-17 23:18:16,736 [INFO] \n",
      "======================================================================\n",
      "2025-12-17 23:18:16,737 [INFO] ANALYTICS REPORT\n",
      "2025-12-17 23:18:16,738 [INFO] ======================================================================\n",
      "\n",
      "2025-12-17 23:18:16,764 [INFO] BASIC STATISTICS:\n",
      "2025-12-17 23:18:16,766 [INFO]    Total Job Postings: 148\n",
      "2025-12-17 23:18:16,771 [INFO]    Unique Companies: 58\n",
      "2025-12-17 23:18:16,773 [INFO]    Unique Positions: 11\n",
      "2025-12-17 23:18:16,777 [INFO]    Unique Locations: 8\n",
      "2025-12-17 23:18:16,778 [INFO] \n",
      "TOP 10 HIRING COMPANIES:\n",
      "2025-12-17 23:18:16,785 [INFO]    Dear Lions and Lionesses: 8 positions\n",
      "2025-12-17 23:18:16,786 [INFO]    Business Group: 6 positions\n",
      "2025-12-17 23:18:16,788 [INFO]    Infosys: 6 positions\n",
      "2025-12-17 23:18:16,790 [INFO]    VIT: 6 positions\n",
      "2025-12-17 23:18:16,793 [INFO]    Lions: 6 positions\n",
      "2025-12-17 23:18:16,796 [INFO]    Clubs & Communities: 4 positions\n",
      "2025-12-17 23:18:16,797 [INFO]    Texas Instruments: 4 positions\n",
      "2025-12-17 23:18:16,801 [INFO]    Embedded Software: 4 positions\n",
      "2025-12-17 23:18:16,803 [INFO]    Google: 4 positions\n",
      "2025-12-17 23:18:16,806 [INFO]    Dear Lions: 4 positions\n",
      "2025-12-17 23:18:16,811 [INFO] \n",
      "TOP 10 JOB POSITIONS:\n",
      "2025-12-17 23:18:16,820 [INFO]    Engineer: 46 openings\n",
      "2025-12-17 23:18:16,823 [INFO]    Intern: 34 openings\n",
      "2025-12-17 23:18:16,831 [INFO]    Developer: 20 openings\n",
      "2025-12-17 23:18:16,834 [INFO]    Senior: 8 openings\n",
      "2025-12-17 23:18:16,835 [INFO]    Associate: 8 openings\n",
      "2025-12-17 23:18:16,837 [INFO]    Analyst: 8 openings\n",
      "2025-12-17 23:18:16,839 [INFO]    Fresher: 6 openings\n",
      "2025-12-17 23:18:16,845 [INFO]    Trainee: 6 openings\n",
      "2025-12-17 23:18:16,848 [INFO]    Junior: 6 openings\n",
      "2025-12-17 23:18:16,850 [INFO]    Consultant: 4 openings\n",
      "2025-12-17 23:18:16,851 [INFO] \n",
      "TOP 10 LOCATIONS:\n",
      "2025-12-17 23:18:16,855 [INFO]    Not Specified: 116 jobs\n",
      "2025-12-17 23:18:16,858 [INFO]    Bangalore: 12 jobs\n",
      "2025-12-17 23:18:16,861 [INFO]    Remote: 6 jobs\n",
      "2025-12-17 23:18:16,863 [INFO]    Gurgaon: 4 jobs\n",
      "2025-12-17 23:18:16,863 [INFO]    Mumbai: 4 jobs\n",
      "2025-12-17 23:18:16,867 [INFO]    Pune: 2 jobs\n",
      "2025-12-17 23:18:16,869 [INFO]    Hyderabad: 2 jobs\n",
      "2025-12-17 23:18:16,869 [INFO]    Hybrid: 2 jobs\n",
      "2025-12-17 23:18:16,877 [INFO] \n",
      "SALARY STATISTICS:\n",
      "2025-12-17 23:18:16,879 [INFO]    Jobs with Salary Info: 4 (2.7%)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u20b9' in position 50: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 942, in <module>\n",
      "    jobs_df, jobs_json = main()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 923, in main\n",
      "    pipeline.generate_analytics()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 881, in generate_analytics\n",
      "    self.logger.info(f\"   Average Salary: ‚Çπ{salary_data.mean()/100000:.2f} LPA\")\n",
      "Message: '   Average Salary: ‚Çπ25.00 LPA'\n",
      "Arguments: ()\n",
      "2025-12-17 23:18:16,881 [INFO]    Average Salary: ‚Çπ25.00 LPA\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u20b9' in position 49: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 942, in <module>\n",
      "    jobs_df, jobs_json = main()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 923, in main\n",
      "    pipeline.generate_analytics()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29228\\735025470.py\", line 882, in generate_analytics\n",
      "    self.logger.info(f\"   Median Salary: ‚Çπ{salary_data.median()/100000:.2f} LPA\")\n",
      "Message: '   Median Salary: ‚Çπ25.00 LPA'\n",
      "Arguments: ()\n",
      "2025-12-17 23:18:16,887 [INFO]    Median Salary: ‚Çπ25.00 LPA\n",
      "2025-12-17 23:18:16,890 [INFO] \n",
      "DATA COMPLETENESS:\n",
      "2025-12-17 23:18:16,890 [INFO]    Average Completeness: 46.02%\n",
      "2025-12-17 23:18:16,893 [INFO] \n",
      "======================================================================\n",
      "\n",
      "2025-12-17 23:18:16,895 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,897 [INFO] PIPELINE COMPLETED SUCCESSFULLY\n",
      "2025-12-17 23:18:16,899 [INFO] ======================================================================\n",
      "2025-12-17 23:18:16,899 [INFO] Output Files:\n",
      "2025-12-17 23:18:16,902 [INFO]    - CSV: structured_job_postings.csv\n",
      "2025-12-17 23:18:16,904 [INFO]    - JSON: structured_job_postings.json\n",
      "2025-12-17 23:18:16,904 [INFO]    - Log: entity_structuring.log\n",
      "2025-12-17 23:18:16,904 [INFO] ======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE STRUCTURED JOB POSTING:\n",
      "======================================================================\n",
      "job_id: 192127511fd4e321_JOB_1\n",
      "email_id: 192127511fd4e321\n",
      "company_name: Glassdoor\n",
      "company_canonical: GLASSDOOR\n",
      "company_confidence: 0.85\n",
      "position_title: Intern\n",
      "position_level: Intern\n",
      "position_confidence: 0.85\n",
      "skills_required: Machine Learning\n",
      "skills_count: 1\n",
      "education_required: B.E, PHD\n",
      "experience_min_years: 0\n",
      "experience_max_years: 0\n",
      "experience_type: Fresher\n",
      "location_city: Gurgaon\n",
      "location_state: Not Specified\n",
      "work_mode: On-site\n",
      "location_confidence: 0.8\n",
      "salary_min: 2500000\n",
      "salary_max: 2500000\n",
      "salary_currency: INR\n",
      "salary_period: annual\n",
      "salary_raw_text: 25lpa\n",
      "salary_confidence: 0.85\n",
      "application_deadline: None\n",
      "apply_link: None\n",
      "contact_email: None\n",
      "completeness_score: 0.6\n",
      "extraction_timestamp: 2025-12-09T23:10:12.261894\n",
      "source_subject: God bless you\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 3: Entity Structuring Pipeline with Incremental Processing\n",
    "Refactored to use configuration file and support incremental processing.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Set, Any, Optional, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass, field, asdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from config_manager import ConfigurationManager\n",
    "\n",
    "# ===================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ===================================================================\n",
    "logger = logging.getLogger(\"EntityStructuring\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# DATA CLASSES FOR STRUCTURED ENTITIES\n",
    "# ===================================================================\n",
    "\n",
    "@dataclass\n",
    "class Company:\n",
    "    \"\"\"Structured company entity with normalization.\"\"\"\n",
    "    name: str\n",
    "    canonical_name: str\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize company data.\"\"\"\n",
    "        if not self.name:\n",
    "            raise ValueError(\"Company name cannot be empty\")\n",
    "        self.canonical_name = self.canonical_name or self._normalize_name()\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "    \n",
    "    def _normalize_name(self) -> str:\n",
    "        \"\"\"Normalize company name to canonical form.\"\"\"\n",
    "        # Will be loaded from config\n",
    "        return self.name.strip()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\"Structured job position entity.\"\"\"\n",
    "    title: str\n",
    "    level: str = \"Not Specified\"\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize position data.\"\"\"\n",
    "        if not self.title:\n",
    "            raise ValueError(\"Position title cannot be empty\")\n",
    "        self.title = self.title.title()\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Requirements:\n",
    "    \"\"\"Job requirements including skills, education, experience.\"\"\"\n",
    "    skills: List[str] = field(default_factory=list)\n",
    "    education: List[str] = field(default_factory=list)\n",
    "    experience_min: int = 0\n",
    "    experience_max: int = 0\n",
    "    experience_type: str = \"Not Specified\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate requirements.\"\"\"\n",
    "        self.skills = [s.strip().title() for s in self.skills if s.strip()]\n",
    "        self.education = [e.strip().upper() for e in self.education if e.strip()]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Location:\n",
    "    \"\"\"Structured location entity.\"\"\"\n",
    "    city: str\n",
    "    state: str = \"Not Specified\"\n",
    "    work_mode: str = \"On-site\"\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize location.\"\"\"\n",
    "        if not self.city:\n",
    "            raise ValueError(\"City cannot be empty\")\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Compensation:\n",
    "    \"\"\"Structured compensation information.\"\"\"\n",
    "    salary_min: int = 0\n",
    "    salary_max: int = 0\n",
    "    currency: str = \"INR\"\n",
    "    period: str = \"annual\"\n",
    "    raw_text: str = \"\"\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate compensation data.\"\"\"\n",
    "        if self.salary_min > self.salary_max and self.salary_max > 0:\n",
    "            self.salary_min, self.salary_max = self.salary_max, self.salary_min\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Application:\n",
    "    \"\"\"Application details including deadline.\"\"\"\n",
    "    deadline: Optional[str] = None\n",
    "    apply_link: Optional[str] = None\n",
    "    contact_email: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JobPosting:\n",
    "    \"\"\"Complete structured job posting.\"\"\"\n",
    "    job_id: str\n",
    "    email_id: str\n",
    "    company: Company\n",
    "    position: Position\n",
    "    requirements: Requirements\n",
    "    location: Location\n",
    "    compensation: Compensation\n",
    "    application: Application\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            'job_id': self.job_id,\n",
    "            'email_id': self.email_id,\n",
    "            'company': asdict(self.company),\n",
    "            'position': asdict(self.position),\n",
    "            'requirements': asdict(self.requirements),\n",
    "            'location': asdict(self.location),\n",
    "            'compensation': asdict(self.compensation),\n",
    "            'application': asdict(self.application),\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "\n",
    "    \n",
    "    def calculate_completeness(self) -> float:\n",
    "        \"\"\"Calculate how complete this job posting is (0-1).\"\"\"\n",
    "        scores = []\n",
    "        scores.append(1.0 if self.company.name else 0.0)\n",
    "        scores.append(1.0 if self.position.title else 0.0)\n",
    "        scores.append(0.8 if self.requirements.skills else 0.0)\n",
    "        scores.append(0.6 if self.location.city else 0.0)\n",
    "        scores.append(0.5 if self.compensation.salary_max > 0 else 0.0)\n",
    "        scores.append(0.3 if self.requirements.education else 0.0)\n",
    "        scores.append(0.2 if self.application.deadline else 0.0)\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ENTITY NORMALIZER (Config-Driven)\n",
    "# ===================================================================\n",
    "\n",
    "class EntityNormalizer:\n",
    "    \"\"\"Normalize and standardize extracted entities using config.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize normalizer with config.\"\"\"\n",
    "        self.logger = logging.getLogger(\"EntityNormalizer\")\n",
    "        norm_config = config.get(\"normalization\", {})\n",
    "        \n",
    "        self.skill_map = norm_config.get(\"skill_map\", {})\n",
    "        self.degree_map = norm_config.get(\"degree_map\", {})\n",
    "        self.city_map = norm_config.get(\"city_map\", {})\n",
    "        self.company_suffixes = norm_config.get(\"company_suffixes\", [])\n",
    "    \n",
    "    def normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"Normalize skill to canonical form.\"\"\"\n",
    "        skill_lower = skill.lower().strip()\n",
    "        return self.skill_map.get(skill_lower, skill_lower)\n",
    "    \n",
    "    def normalize_degree(self, degree: str) -> str:\n",
    "        \"\"\"Normalize degree to canonical form.\"\"\"\n",
    "        degree_lower = degree.lower().strip()\n",
    "        return self.degree_map.get(degree_lower, degree.upper())\n",
    "    \n",
    "    def normalize_city(self, city: str) -> str:\n",
    "        \"\"\"Normalize city name.\"\"\"\n",
    "        return self.city_map.get(city.lower(), city.title())\n",
    "    \n",
    "    def normalize_company_name(self, name: str) -> str:\n",
    "        \"\"\"Normalize company name.\"\"\"\n",
    "        name_upper = name.upper()\n",
    "        for suffix in self.company_suffixes:\n",
    "            name_upper = name_upper.replace(suffix, '').strip()\n",
    "        return name_upper.strip()\n",
    "\n",
    "    \n",
    "    def normalize_skills_list(self, skills: List[str]) -> List[str]:\n",
    "        \"\"\"Normalize a list of skills.\"\"\"\n",
    "        normalized = []\n",
    "        seen = set()\n",
    "        for skill in skills:\n",
    "            norm_skill = self.normalize_skill(skill)\n",
    "            if norm_skill not in seen:\n",
    "                normalized.append(norm_skill)\n",
    "                seen.add(norm_skill)\n",
    "        return normalized\n",
    "    \n",
    "    def normalize_degrees_list(self, degrees: List[str]) -> List[str]:\n",
    "        \"\"\"Normalize a list of degrees.\"\"\"\n",
    "        normalized = []\n",
    "        seen = set()\n",
    "        for degree in degrees:\n",
    "            norm_degree = self.normalize_degree(degree)\n",
    "            if norm_degree not in seen:\n",
    "                normalized.append(norm_degree)\n",
    "                seen.add(norm_degree)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PARSERS (Config-Driven)\n",
    "# ===================================================================\n",
    "\n",
    "class SalaryParser:\n",
    "    \"\"\"Parse salary information using config patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize with config.\"\"\"\n",
    "        self.logger = logging.getLogger(\"SalaryParser\")\n",
    "        salary_config = config.get(\"salary_parsing\", {})\n",
    "        \n",
    "        self.patterns = []\n",
    "        for pattern_config in salary_config.get(\"patterns\", []):\n",
    "            self.patterns.append((\n",
    "                pattern_config.get(\"pattern\", \"\"),\n",
    "                pattern_config.get(\"name\", \"\"),\n",
    "                pattern_config.get(\"confidence\", 0.8)\n",
    "            ))\n",
    "        \n",
    "        self.default_currency = salary_config.get(\"default_currency\", \"INR\")\n",
    "        self.default_period = salary_config.get(\"default_period\", \"annual\")\n",
    "    \n",
    "    def parse(self, text: str) -> Optional[Compensation]:\n",
    "        \"\"\"Parse salary from text.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return None\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        try:\n",
    "            for pattern, pattern_type, confidence in self.patterns:\n",
    "                match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return self._parse_match(match, pattern_type, confidence)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing salary '{text}': {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    \n",
    "    def _parse_match(self, match, pattern_type: str, confidence: float) -> Optional[Compensation]:\n",
    "        \"\"\"Parse regex match into Compensation object.\"\"\"\n",
    "        try:\n",
    "            if pattern_type == 'lpa_range':\n",
    "                min_val = float(match.group(1))\n",
    "                max_val = float(match.group(2))\n",
    "                return Compensation(\n",
    "                    salary_min=int(min_val * 100000),\n",
    "                    salary_max=int(max_val * 100000),\n",
    "                    currency=self.default_currency,\n",
    "                    period=self.default_period,\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=confidence\n",
    "                )\n",
    "            \n",
    "            elif pattern_type in ['lpa_single', 'ctc']:\n",
    "                val = float(match.group(1))\n",
    "                return Compensation(\n",
    "                    salary_min=int(val * 100000),\n",
    "                    salary_max=int(val * 100000),\n",
    "                    currency=self.default_currency,\n",
    "                    period=self.default_period,\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=confidence\n",
    "                )\n",
    "            \n",
    "            elif pattern_type == 'monthly':\n",
    "                val = float(match.group(1))\n",
    "                annual = val * 1000 * 12 if val < 1000 else val * 12\n",
    "                return Compensation(\n",
    "                    salary_min=int(annual),\n",
    "                    salary_max=int(annual),\n",
    "                    currency=self.default_currency,\n",
    "                    period=self.default_period,\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=confidence\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing match: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class ExperienceParser:\n",
    "    \"\"\"Parse experience requirements using config patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize with config.\"\"\"\n",
    "        self.logger = logging.getLogger(\"ExperienceParser\")\n",
    "        exp_config = config.get(\"experience_parsing\", {})\n",
    "        self.patterns = exp_config.get(\"patterns\", [])\n",
    "        \n",
    "        exp_types = config.get(\"experience_types\", {})\n",
    "        self.fresher_keywords = exp_types.get(\"fresher_keywords\", [])\n",
    "\n",
    "    \n",
    "    def parse(self, text: str) -> Tuple[int, int]:\n",
    "        \"\"\"Parse experience requirement.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return (0, 0)\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for fresher keywords\n",
    "        if any(word in text_lower for word in self.fresher_keywords):\n",
    "            return (0, 0)\n",
    "        \n",
    "        try:\n",
    "            for pattern in self.patterns:\n",
    "                match = re.search(pattern, text_lower)\n",
    "                if match:\n",
    "                    groups = match.groups()\n",
    "                    if len(groups) == 2:\n",
    "                        return (int(groups[0]), int(groups[1]))\n",
    "                    elif len(groups) == 1:\n",
    "                        val = int(groups[0])\n",
    "                        return (val, val)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing experience '{text}': {e}\")\n",
    "        \n",
    "        return (0, 0)\n",
    "\n",
    "\n",
    "class DeadlineParser:\n",
    "    \"\"\"Parse deadline dates using config patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize with config.\"\"\"\n",
    "        self.logger = logging.getLogger(\"DeadlineParser\")\n",
    "        deadline_config = config.get(\"deadline_parsing\", {})\n",
    "        \n",
    "        self.date_patterns = []\n",
    "        for pattern_config in deadline_config.get(\"date_patterns\", []):\n",
    "            self.date_patterns.append((\n",
    "                pattern_config.get(\"pattern\", \"\"),\n",
    "                pattern_config.get(\"format\", \"\")\n",
    "            ))\n",
    "        \n",
    "        self.relative_keywords = deadline_config.get(\"relative_keywords\", {})\n",
    "    \n",
    "    def parse(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Parse deadline from text.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Try standard date formats\n",
    "            for pattern, fmt in self.date_patterns:\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    date_str = match.group(0)\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_str, fmt)\n",
    "                        return dt.strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Handle relative dates\n",
    "            text_lower = text.lower()\n",
    "            for keyword, days_offset in self.relative_keywords.items():\n",
    "                if keyword in text_lower:\n",
    "                    future_date = datetime.now() + timedelta(days=days_offset)\n",
    "                    return future_date.strftime('%Y-%m-%d')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing deadline '{text}': {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# INCREMENTAL PROCESSING STATE MANAGER\n",
    "# ===================================================================\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"Manage incremental processing state.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize state manager.\"\"\"\n",
    "        self.logger = logging.getLogger(\"StateManager\")\n",
    "        inc_config = config.get(\"incremental_processing\", {})\n",
    "        \n",
    "        self.enabled = inc_config.get(\"enabled\", True)\n",
    "        self.state_dir = inc_config.get(\"state_directory\", \"state\")\n",
    "        self.state_file = inc_config.get(\"state_file\", \"processed_message_ids.txt\")\n",
    "        self.checkpoint_interval = inc_config.get(\"checkpoint_interval\", 50)\n",
    "        self.force_full_reprocess = inc_config.get(\"force_full_reprocess\", False)\n",
    "        \n",
    "        # Create state directory if it doesn't exist\n",
    "        os.makedirs(self.state_dir, exist_ok=True)\n",
    "        \n",
    "        self.state_path = os.path.join(self.state_dir, self.state_file)\n",
    "        self.processed_ids: Set[str] = set()\n",
    "        \n",
    "        if self.enabled and not self.force_full_reprocess:\n",
    "            self.load_state()\n",
    "    \n",
    "    def load_state(self) -> Set[str]:\n",
    "        \"\"\"Load processed message IDs from state file.\"\"\"\n",
    "        if os.path.exists(self.state_path):\n",
    "            try:\n",
    "                with open(self.state_path, 'r', encoding='utf-8') as f:\n",
    "                    self.processed_ids = set(line.strip() for line in f if line.strip())\n",
    "                self.logger.info(f\"Loaded {len(self.processed_ids)} processed message IDs from state\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading state: {e}\")\n",
    "                self.processed_ids = set()\n",
    "        else:\n",
    "            self.logger.info(\"No existing state file found, starting fresh\")\n",
    "            self.processed_ids = set()\n",
    "        \n",
    "        return self.processed_ids\n",
    "    \n",
    "    def save_state(self, new_ids: Set[str]) -> None:\n",
    "        \"\"\"Save processed message IDs to state file.\"\"\"\n",
    "        try:\n",
    "            self.processed_ids.update(new_ids)\n",
    "            with open(self.state_path, 'w', encoding='utf-8') as f:\n",
    "                for msg_id in sorted(self.processed_ids):\n",
    "                    f.write(f\"{msg_id}\\n\")\n",
    "            self.logger.info(f\"Saved {len(self.processed_ids)} processed message IDs to state\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving state: {e}\")\n",
    "    \n",
    "    def is_processed(self, message_id: str) -> bool:\n",
    "        \"\"\"Check if message ID has been processed.\"\"\"\n",
    "        return message_id in self.processed_ids\n",
    "    \n",
    "    def get_unprocessed_emails(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Filter dataframe to only unprocessed emails.\"\"\"\n",
    "        if not self.enabled or self.force_full_reprocess:\n",
    "            self.logger.info(\"Incremental processing disabled, processing all emails\")\n",
    "            return df\n",
    "        \n",
    "        # Filter out already processed emails\n",
    "        unprocessed = df[~df['MessageId'].isin(self.processed_ids)]\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Total emails: {len(df)}, \"\n",
    "            f\"Already processed: {len(self.processed_ids)}, \"\n",
    "            f\"New to process: {len(unprocessed)}\"\n",
    "        )\n",
    "        \n",
    "        return unprocessed\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# RELATIONSHIP EXTRACTOR\n",
    "# ===================================================================\n",
    "\n",
    "class RelationshipExtractor:\n",
    "    \"\"\"Extract relationships between entities.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initialize relationship extractor.\"\"\"\n",
    "        self.logger = logging.getLogger(\"RelationshipExtractor\")\n",
    "        self.config = config\n",
    "        self.normalizer = EntityNormalizer(config)\n",
    "        self.salary_parser = SalaryParser(config)\n",
    "        self.experience_parser = ExperienceParser(config)\n",
    "        self.deadline_parser = DeadlineParser(config)\n",
    "        \n",
    "        # Get processing limits from config\n",
    "        proc_config = config.get(\"processing\", {})\n",
    "        self.max_companies = proc_config.get(\"max_companies_per_email\", 3)\n",
    "        self.max_positions = proc_config.get(\"max_positions_per_email\", 3)\n",
    "        self.max_jobs = proc_config.get(\"max_jobs_per_email\", 5)\n",
    "    \n",
    "    def extract_job_postings(self, row: pd.Series, email_id: str) -> List[JobPosting]:\n",
    "        \"\"\"Extract structured job postings from a DataFrame row.\"\"\"\n",
    "        job_postings = []\n",
    "        \n",
    "        try:\n",
    "            # Parse entities from row\n",
    "            companies = self._parse_list(row.get('companies_extracted', ''))\n",
    "            positions = self._parse_list(row.get('positions_extracted', ''))\n",
    "            skills = self._parse_list(row.get('skills_extracted', ''))\n",
    "            locations = self._parse_list(row.get('locations_extracted', ''))\n",
    "            salary_texts = self._parse_list(row.get('salary_info', ''))\n",
    "            experience_texts = self._parse_list(row.get('experience_required', ''))\n",
    "            degrees = self._parse_list(row.get('degrees_required', ''))\n",
    "            \n",
    "            # If no companies or positions, return empty\n",
    "            if not companies or not positions:\n",
    "                self.logger.debug(f\"No companies or positions found in {email_id}\")\n",
    "                return []\n",
    "            \n",
    "            # Normalize entities\n",
    "            skills = self.normalizer.normalize_skills_list(skills)\n",
    "            degrees = self.normalizer.normalize_degrees_list(degrees)\n",
    "            \n",
    "            # Create job postings\n",
    "            for idx, (company_name, position_title) in enumerate(\n",
    "                self._create_pairs(companies, positions)\n",
    "            ):\n",
    "                job_id = f\"{email_id}_JOB_{idx+1}\"\n",
    "                \n",
    "                try:\n",
    "                    # Create company\n",
    "                    company = Company(\n",
    "                        name=company_name,\n",
    "                        canonical_name=self.normalizer.normalize_company_name(company_name),\n",
    "                        confidence=0.85\n",
    "                    )\n",
    "                    \n",
    "                    # Create position\n",
    "                    position = Position(\n",
    "                        title=position_title,\n",
    "                        confidence=0.85\n",
    "                    )\n",
    "                    \n",
    "                    # Parse experience\n",
    "                    exp_min, exp_max = 0, 0\n",
    "                    if experience_texts:\n",
    "                        exp_min, exp_max = self.experience_parser.parse(experience_texts[0])\n",
    "                    \n",
    "                    requirements = Requirements(\n",
    "                        skills=skills,\n",
    "                        education=degrees,\n",
    "                        experience_min=exp_min,\n",
    "                        experience_max=exp_max\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    # Parse location\n",
    "                    location_city = locations[idx] if idx < len(locations) else (\n",
    "                        locations[0] if locations else \"Not Specified\"\n",
    "                    )\n",
    "                    \n",
    "                    location = Location(\n",
    "                        city=self.normalizer.normalize_city(location_city),\n",
    "                        confidence=0.8\n",
    "                    )\n",
    "                    \n",
    "                    # Parse salary\n",
    "                    compensation = Compensation()\n",
    "                    if salary_texts:\n",
    "                        parsed_salary = self.salary_parser.parse(salary_texts[0])\n",
    "                        if parsed_salary:\n",
    "                            compensation = parsed_salary\n",
    "                    \n",
    "                    # Parse deadline\n",
    "                    deadline = None\n",
    "                    deadline_col = row.get('deadline', '')\n",
    "                    if deadline_col:\n",
    "                        deadline = self.deadline_parser.parse(str(deadline_col))\n",
    "                    \n",
    "                    application = Application(deadline=deadline)\n",
    "                    \n",
    "                    # Create job posting\n",
    "                    job_posting = JobPosting(\n",
    "                        job_id=job_id,\n",
    "                        email_id=email_id,\n",
    "                        company=company,\n",
    "                        position=position,\n",
    "                        requirements=requirements,\n",
    "                        location=location,\n",
    "                        compensation=compensation,\n",
    "                        application=application,\n",
    "                        metadata={\n",
    "                            'extraction_timestamp': datetime.now().isoformat(),\n",
    "                            'completeness_score': 0.0,\n",
    "                            'source_subject': str(row.get('Subject', ''))[:100]\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate completeness\n",
    "                    job_posting.metadata['completeness_score'] = job_posting.calculate_completeness()\n",
    "                    \n",
    "                    job_postings.append(job_posting)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error creating job posting {job_id}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting job postings from {email_id}: {e}\")\n",
    "        \n",
    "        return job_postings\n",
    "    \n",
    "    def _parse_list(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse comma-separated string into list.\"\"\"\n",
    "        if not text or pd.isna(text) or text == '':\n",
    "            return []\n",
    "        return [item.strip() for item in str(text).split(',') if item.strip()]\n",
    "    \n",
    "    def _create_pairs(self, companies: List[str], positions: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Create company-position pairs.\"\"\"\n",
    "        if len(companies) == len(positions):\n",
    "            return list(zip(companies, positions))\n",
    "        \n",
    "        # Create combinations but limit to avoid explosion\n",
    "        pairs = []\n",
    "        for company in companies[:self.max_companies]:\n",
    "            for position in positions[:self.max_positions]:\n",
    "                pairs.append((company, position))\n",
    "        \n",
    "        return pairs[:self.max_jobs]\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN STRUCTURING PIPELINE\n",
    "# ===================================================================\n",
    "\n",
    "class EntityStructuringPipeline:\n",
    "    \"\"\"Main pipeline for entity structuring with incremental processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = \"config.json\"):\n",
    "        \"\"\"Initialize pipeline with configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(\"StructuringPipeline\")\n",
    "        \n",
    "        # Load configuration\n",
    "        self.config_manager = ConfigurationManager(config_path)\n",
    "        self.config = self.config_manager.load_config()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.state_manager = StateManager(self.config)\n",
    "        self.relationship_extractor = RelationshipExtractor(self.config)\n",
    "        \n",
    "        # Get config values\n",
    "        io_config = self.config.get(\"input_output\", {})\n",
    "        self.input_file = io_config.get(\"input_file\", \"../Phase 2/relevant_placement_emails.csv\")\n",
    "        self.output_csv = io_config.get(\"output_csv\", \"structured_job_postings.csv\")\n",
    "        self.output_json = io_config.get(\"output_json\", \"structured_job_postings.json\")\n",
    "        \n",
    "        proc_config = self.config.get(\"processing\", {})\n",
    "        self.min_completeness = proc_config.get(\"min_completeness_score\", 0.3)\n",
    "        self.enable_analytics = proc_config.get(\"enable_analytics\", True)\n",
    "        \n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(\"üèóÔ∏è  ENTITY STRUCTURING PIPELINE INITIALIZED\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "    \n",
    "    def process_dataset(self) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "        \"\"\"Process dataset with incremental processing support.\"\"\"\n",
    "        self.logger.info(f\"\\nLoading dataset: {self.input_file}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(self.input_file)\n",
    "            total_emails = len(df)\n",
    "            self.logger.info(f\"Loaded {total_emails} emails\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Get unprocessed emails\n",
    "        unprocessed_df = self.state_manager.get_unprocessed_emails(df)\n",
    "        \n",
    "        if len(unprocessed_df) == 0:\n",
    "            self.logger.info(\"No new emails to process!\")\n",
    "            return self._load_existing_results()\n",
    "        \n",
    "        # Load existing results\n",
    "        existing_jobs = self._load_existing_jobs()\n",
    "        \n",
    "        # Process new emails\n",
    "        self.logger.info(f\"\\nProcessing {len(unprocessed_df)} new emails...\")\n",
    "        \n",
    "        new_job_postings = []\n",
    "        processed_message_ids = set()\n",
    "        stats = {\n",
    "            'total_new_emails': len(unprocessed_df),\n",
    "            'emails_with_jobs': 0,\n",
    "            'total_new_jobs': 0\n",
    "        }\n",
    "\n",
    "        \n",
    "        for idx, row in unprocessed_df.iterrows():\n",
    "            email_id = row.get('MessageId', f\"EMAIL_{idx}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract job postings from this email\n",
    "                job_postings = self.relationship_extractor.extract_job_postings(row, email_id)\n",
    "                \n",
    "                # Filter by completeness score\n",
    "                job_postings = [\n",
    "                    job for job in job_postings \n",
    "                    if job.calculate_completeness() >= self.min_completeness\n",
    "                ]\n",
    "                \n",
    "                if job_postings:\n",
    "                    stats['emails_with_jobs'] += 1\n",
    "                    stats['total_new_jobs'] += len(job_postings)\n",
    "                    \n",
    "                    for job in job_postings:\n",
    "                        new_job_postings.append(job.to_dict())\n",
    "                \n",
    "                # Mark as processed\n",
    "                processed_message_ids.add(email_id)\n",
    "                \n",
    "                # Checkpoint every N emails\n",
    "                if len(processed_message_ids) % self.state_manager.checkpoint_interval == 0:\n",
    "                    self.state_manager.save_state(processed_message_ids)\n",
    "                    self.logger.info(\n",
    "                        f\"   Checkpoint: {len(processed_message_ids)}/{len(unprocessed_df)} emails | \"\n",
    "                        f\"Jobs extracted: {stats['total_new_jobs']}\"\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing email {email_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Final state save\n",
    "        self.state_manager.save_state(processed_message_ids)\n",
    "        \n",
    "        # Combine with existing jobs\n",
    "        all_job_postings = existing_jobs + new_job_postings\n",
    "        \n",
    "        # Log results\n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"PROCESSING COMPLETE\")\n",
    "        self.logger.info(f\"{'='*70}\")\n",
    "        self.logger.info(f\"New Emails Processed: {stats['total_new_emails']}\")\n",
    "        self.logger.info(f\"New Emails with Jobs: {stats['emails_with_jobs']}\")\n",
    "        self.logger.info(f\"New Job Postings: {stats['total_new_jobs']}\")\n",
    "        self.logger.info(f\"Total Job Postings: {len(all_job_postings)}\")\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Save outputs\n",
    "        self._save_outputs(all_job_postings)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        jobs_df = self._create_dataframe(all_job_postings)\n",
    "        \n",
    "        return jobs_df, all_job_postings\n",
    "\n",
    "    \n",
    "    def _load_existing_jobs(self) -> List[Dict]:\n",
    "        \"\"\"Load existing job postings from JSON file.\"\"\"\n",
    "        if os.path.exists(self.output_json):\n",
    "            try:\n",
    "                with open(self.output_json, 'r', encoding='utf-8') as f:\n",
    "                    existing = json.load(f)\n",
    "                self.logger.info(f\"Loaded {len(existing)} existing job postings\")\n",
    "                return existing\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading existing jobs: {e}\")\n",
    "                return []\n",
    "        return []\n",
    "    \n",
    "    def _load_existing_results(self) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "        \"\"\"Load existing results when no new emails to process.\"\"\"\n",
    "        existing_jobs = self._load_existing_jobs()\n",
    "        jobs_df = self._create_dataframe(existing_jobs)\n",
    "        return jobs_df, existing_jobs\n",
    "    \n",
    "    def _save_outputs(self, job_postings: List[Dict]) -> None:\n",
    "        \"\"\"Save structured job postings to CSV and JSON.\"\"\"\n",
    "        try:\n",
    "            # Save JSON\n",
    "            self.logger.info(f\"Saving JSON: {self.output_json}\")\n",
    "            with open(self.output_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(job_postings, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"Saved {len(job_postings)} jobs to JSON\")\n",
    "            \n",
    "            # Save CSV\n",
    "            self.logger.info(f\"Saving CSV: {self.output_csv}\")\n",
    "            df = self._create_dataframe(job_postings)\n",
    "            df.to_csv(self.output_csv, index=False)\n",
    "            self.logger.info(f\"Saved {len(df)} jobs to CSV\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving outputs: {e}\")\n",
    "    \n",
    "    def _create_dataframe(self, job_postings: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Create flattened DataFrame from structured job postings.\"\"\"\n",
    "        flattened = []\n",
    "        \n",
    "        for job in job_postings:\n",
    "            try:\n",
    "                flat_job = {\n",
    "                    'job_id': job['job_id'],\n",
    "                    'email_id': job['email_id'],\n",
    "                    'company_name': job['company']['name'],\n",
    "                    'company_canonical': job['company']['canonical_name'],\n",
    "                    'company_confidence': job['company']['confidence'],\n",
    "                    'position_title': job['position']['title'],\n",
    "                    'position_level': job['position']['level'],\n",
    "                    'position_confidence': job['position']['confidence'],\n",
    "                    'skills_required': ', '.join(job['requirements']['skills']),\n",
    "                    'skills_count': len(job['requirements']['skills']),\n",
    "                    'education_required': ', '.join(job['requirements']['education']),\n",
    "                    'experience_min_years': job['requirements']['experience_min'],\n",
    "                    'experience_max_years': job['requirements']['experience_max'],\n",
    "                    'experience_type': job['requirements']['experience_type'],\n",
    "                    'location_city': job['location']['city'],\n",
    "                    'location_state': job['location']['state'],\n",
    "                    'work_mode': job['location']['work_mode'],\n",
    "                    'location_confidence': job['location']['confidence'],\n",
    "                    'salary_min': job['compensation']['salary_min'],\n",
    "                    'salary_max': job['compensation']['salary_max'],\n",
    "                    'salary_currency': job['compensation']['currency'],\n",
    "                    'salary_period': job['compensation']['period'],\n",
    "                    'salary_raw_text': job['compensation']['raw_text'],\n",
    "                    'salary_confidence': job['compensation']['confidence'],\n",
    "                    'application_deadline': job['application']['deadline'],\n",
    "                    'apply_link': job['application']['apply_link'],\n",
    "                    'contact_email': job['application']['contact_email'],\n",
    "                    'completeness_score': job['metadata']['completeness_score'],\n",
    "                    'extraction_timestamp': job['metadata']['extraction_timestamp'],\n",
    "                    'source_subject': job['metadata']['source_subject']\n",
    "                }\n",
    "                \n",
    "                flattened.append(flat_job)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error flattening job {job.get('job_id')}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(flattened)\n",
    "\n",
    "    \n",
    "    def generate_analytics(self) -> None:\n",
    "        \"\"\"Generate analytics report on structured data.\"\"\"\n",
    "        if not self.enable_analytics:\n",
    "            self.logger.info(\"Analytics disabled in configuration\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"ANALYTICS REPORT\")\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(self.output_csv)\n",
    "            \n",
    "            # Basic statistics\n",
    "            self.logger.info(f\"BASIC STATISTICS:\")\n",
    "            self.logger.info(f\"   Total Job Postings: {len(df)}\")\n",
    "            self.logger.info(f\"   Unique Companies: {df['company_name'].nunique()}\")\n",
    "            self.logger.info(f\"   Unique Positions: {df['position_title'].nunique()}\")\n",
    "            self.logger.info(f\"   Unique Locations: {df['location_city'].nunique()}\")\n",
    "            \n",
    "            # Top companies\n",
    "            self.logger.info(f\"\\nTOP 10 HIRING COMPANIES:\")\n",
    "            top_companies = df['company_name'].value_counts().head(10)\n",
    "            for company, count in top_companies.items():\n",
    "                self.logger.info(f\"   {company}: {count} positions\")\n",
    "            \n",
    "            # Top positions\n",
    "            self.logger.info(f\"\\nTOP 10 JOB POSITIONS:\")\n",
    "            top_positions = df['position_title'].value_counts().head(10)\n",
    "            for position, count in top_positions.items():\n",
    "                self.logger.info(f\"   {position}: {count} openings\")\n",
    "            \n",
    "            # Top locations\n",
    "            self.logger.info(f\"\\nTOP 10 LOCATIONS:\")\n",
    "            top_locations = df['location_city'].value_counts().head(10)\n",
    "            for location, count in top_locations.items():\n",
    "                self.logger.info(f\"   {location}: {count} jobs\")\n",
    "            \n",
    "            # Salary statistics\n",
    "            salary_data = df[df['salary_max'] > 0]['salary_max']\n",
    "            if len(salary_data) > 0:\n",
    "                self.logger.info(f\"\\nSALARY STATISTICS:\")\n",
    "                self.logger.info(f\"   Jobs with Salary Info: {len(salary_data)} ({len(salary_data)/len(df)*100:.1f}%)\")\n",
    "                self.logger.info(f\"   Average Salary: ‚Çπ{salary_data.mean()/100000:.2f} LPA\")\n",
    "                self.logger.info(f\"   Median Salary: ‚Çπ{salary_data.median()/100000:.2f} LPA\")\n",
    "            \n",
    "            # Completeness\n",
    "            self.logger.info(f\"\\nDATA COMPLETENESS:\")\n",
    "            avg_completeness = df['completeness_score'].mean()\n",
    "            self.logger.info(f\"   Average Completeness: {avg_completeness:.2%}\")\n",
    "            \n",
    "            self.logger.info(f\"\\n{'='*70}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating analytics: {e}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"entity_structuring.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"STARTING ENTITY STRUCTURING PIPELINE\")\n",
    "    logger.info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize pipeline\n",
    "        pipeline = EntityStructuringPipeline()\n",
    "        \n",
    "        # Process dataset\n",
    "        jobs_df, jobs_list = pipeline.process_dataset()\n",
    "        \n",
    "        # Generate analytics\n",
    "        pipeline.generate_analytics()\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Output Files:\")\n",
    "        logger.info(f\"   - CSV: {pipeline.output_csv}\")\n",
    "        logger.info(f\"   - JSON: {pipeline.output_json}\")\n",
    "        logger.info(f\"   - Log: entity_structuring.log\")\n",
    "        logger.info(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return jobs_df, jobs_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"PIPELINE FAILED: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jobs_df, jobs_json = main()\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSAMPLE STRUCTURED JOB POSTING:\")\n",
    "    print(\"=\"*70)\n",
    "    if len(jobs_df) > 0:\n",
    "        sample = jobs_df.iloc[0].to_dict()\n",
    "        for key, value in sample.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
