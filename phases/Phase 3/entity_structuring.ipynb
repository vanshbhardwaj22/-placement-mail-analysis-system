{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da97daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PHASE 3: ENTITY STRUCTURING & RELATIONSHIP EXTRACTION PIPELINE\n",
    "# Production-Level Code with Error Handling and Comprehensive Logging\n",
    "# ===================================================================\n",
    "# Purpose: Transform flat entity strings into structured job postings\n",
    "#          with proper relationships and validation\n",
    "# ===================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Set, Any, Optional, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass, field, asdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ===================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"entity_structuring.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"EntityStructuring\")\n",
    "\n",
    "# ===================================================================\n",
    "# DATA CLASSES FOR STRUCTURED ENTITIES\n",
    "# ===================================================================\n",
    "\n",
    "@dataclass\n",
    "class Company:\n",
    "    \"\"\"Structured company entity with normalization.\"\"\"\n",
    "    name: str\n",
    "    canonical_name: str\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize company data.\"\"\"\n",
    "        if not self.name:\n",
    "            raise ValueError(\"Company name cannot be empty\")\n",
    "        self.canonical_name = self.canonical_name or self._normalize_name()\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "    \n",
    "    def _normalize_name(self) -> str:\n",
    "        \"\"\"Normalize company name to canonical form.\"\"\"\n",
    "        name = self.name.upper()\n",
    "        # Remove common suffixes\n",
    "        suffixes = ['PVT LTD', 'PVT. LTD.', 'PRIVATE LIMITED', 'LIMITED', \n",
    "                   'LTD', 'INC', 'CORP', 'CORPORATION', 'LLC']\n",
    "        for suffix in suffixes:\n",
    "            name = name.replace(suffix, '').strip()\n",
    "        return name.strip()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\"Structured job position entity.\"\"\"\n",
    "    title: str\n",
    "    level: str = \"Not Specified\"  # Junior, Mid, Senior, Lead\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize position data.\"\"\"\n",
    "        if not self.title:\n",
    "            raise ValueError(\"Position title cannot be empty\")\n",
    "        self.title = self.title.title()  # Capitalize properly\n",
    "        self.level = self._determine_level()\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "    \n",
    "    def _determine_level(self) -> str:\n",
    "        \"\"\"Determine seniority level from title.\"\"\"\n",
    "        title_lower = self.title.lower()\n",
    "        if any(word in title_lower for word in ['senior', 'lead', 'principal', 'staff']):\n",
    "            return \"Senior\"\n",
    "        elif any(word in title_lower for word in ['junior', 'associate', 'entry']):\n",
    "            return \"Junior\"\n",
    "        elif any(word in title_lower for word in ['intern', 'trainee']):\n",
    "            return \"Intern\"\n",
    "        elif any(word in title_lower for word in ['manager', 'head', 'director']):\n",
    "            return \"Manager\"\n",
    "        return \"Mid\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Requirements:\n",
    "    \"\"\"Job requirements including skills, education, experience.\"\"\"\n",
    "    skills: List[str] = field(default_factory=list)\n",
    "    education: List[str] = field(default_factory=list)\n",
    "    experience_min: int = 0\n",
    "    experience_max: int = 0\n",
    "    experience_type: str = \"Not Specified\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate requirements.\"\"\"\n",
    "        self.skills = [s.strip().title() for s in self.skills if s.strip()]\n",
    "        self.education = [e.strip().upper() for e in self.education if e.strip()]\n",
    "        self.experience_type = self._determine_experience_type()\n",
    "    \n",
    "    def _determine_experience_type(self) -> str:\n",
    "        \"\"\"Determine experience category.\"\"\"\n",
    "        if self.experience_max == 0:\n",
    "            return \"Fresher\"\n",
    "        elif self.experience_max <= 2:\n",
    "            return \"Entry Level\"\n",
    "        elif self.experience_max <= 5:\n",
    "            return \"Mid Level\"\n",
    "        else:\n",
    "            return \"Senior Level\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Location:\n",
    "    \"\"\"Structured location entity.\"\"\"\n",
    "    city: str\n",
    "    state: str = \"Not Specified\"\n",
    "    work_mode: str = \"On-site\"  # On-site, Remote, Hybrid\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and normalize location.\"\"\"\n",
    "        if not self.city:\n",
    "            raise ValueError(\"City cannot be empty\")\n",
    "        self.city = self._normalize_city()\n",
    "        self.work_mode = self._determine_work_mode()\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "    \n",
    "    def _normalize_city(self) -> str:\n",
    "        \"\"\"Normalize city name to canonical form.\"\"\"\n",
    "        # City name mappings\n",
    "        city_map = {\n",
    "            'bangalore': 'Bangalore',\n",
    "            'bengaluru': 'Bangalore',\n",
    "            'blr': 'Bangalore',\n",
    "            'mumbai': 'Mumbai',\n",
    "            'bombay': 'Mumbai',\n",
    "            'delhi': 'Delhi',\n",
    "            'new delhi': 'Delhi',\n",
    "            'ncr': 'Delhi NCR',\n",
    "            'gurgaon': 'Gurgaon',\n",
    "            'gurugram': 'Gurgaon',\n",
    "            'hyderabad': 'Hyderabad',\n",
    "            'pune': 'Pune',\n",
    "            'chennai': 'Chennai',\n",
    "            'kolkata': 'Kolkata',\n",
    "            'calcutta': 'Kolkata'\n",
    "        }\n",
    "        return city_map.get(self.city.lower(), self.city.title())\n",
    "    \n",
    "    def _determine_work_mode(self) -> str:\n",
    "        \"\"\"Determine work mode from city name.\"\"\"\n",
    "        city_lower = self.city.lower()\n",
    "        if any(word in city_lower for word in ['remote', 'wfh', 'work from home', 'anywhere']):\n",
    "            return \"Remote\"\n",
    "        elif 'hybrid' in city_lower:\n",
    "            return \"Hybrid\"\n",
    "        return \"On-site\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Compensation:\n",
    "    \"\"\"Structured compensation information.\"\"\"\n",
    "    salary_min: int = 0\n",
    "    salary_max: int = 0\n",
    "    currency: str = \"INR\"\n",
    "    period: str = \"annual\"  # annual, monthly\n",
    "    raw_text: str = \"\"\n",
    "    confidence: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate compensation data.\"\"\"\n",
    "        if self.salary_min > self.salary_max and self.salary_max > 0:\n",
    "            self.salary_min, self.salary_max = self.salary_max, self.salary_min\n",
    "        self.confidence = max(0.0, min(1.0, self.confidence))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Application:\n",
    "    \"\"\"Application details including deadline.\"\"\"\n",
    "    deadline: Optional[str] = None\n",
    "    apply_link: Optional[str] = None\n",
    "    contact_email: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate application data.\"\"\"\n",
    "        if self.deadline:\n",
    "            self.deadline = self._normalize_deadline()\n",
    "    \n",
    "    def _normalize_deadline(self) -> Optional[str]:\n",
    "        \"\"\"Normalize deadline to ISO format.\"\"\"\n",
    "        try:\n",
    "            # Try parsing common formats\n",
    "            for fmt in ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y', '%d/%m/%y']:\n",
    "                try:\n",
    "                    dt = datetime.strptime(self.deadline, fmt)\n",
    "                    return dt.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "            return self.deadline\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JobPosting:\n",
    "    \"\"\"Complete structured job posting.\"\"\"\n",
    "    job_id: str\n",
    "    email_id: str\n",
    "    company: Company\n",
    "    position: Position\n",
    "    requirements: Requirements\n",
    "    location: Location\n",
    "    compensation: Compensation\n",
    "    application: Application\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            'job_id': self.job_id,\n",
    "            'email_id': self.email_id,\n",
    "            'company': asdict(self.company),\n",
    "            'position': asdict(self.position),\n",
    "            'requirements': asdict(self.requirements),\n",
    "            'location': asdict(self.location),\n",
    "            'compensation': asdict(self.compensation),\n",
    "            'application': asdict(self.application),\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "    \n",
    "    def calculate_completeness(self) -> float:\n",
    "        \"\"\"Calculate how complete this job posting is (0-1).\"\"\"\n",
    "        scores = []\n",
    "        scores.append(1.0 if self.company.name else 0.0)\n",
    "        scores.append(1.0 if self.position.title else 0.0)\n",
    "        scores.append(0.8 if self.requirements.skills else 0.0)\n",
    "        scores.append(0.6 if self.location.city else 0.0)\n",
    "        scores.append(0.5 if self.compensation.salary_max > 0 else 0.0)\n",
    "        scores.append(0.3 if self.requirements.education else 0.0)\n",
    "        scores.append(0.2 if self.application.deadline else 0.0)\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ENTITY NORMALIZER\n",
    "# ===================================================================\n",
    "\n",
    "class EntityNormalizer:\n",
    "    \"\"\"\n",
    "    Normalize and standardize extracted entities.\n",
    "    Handles variations, aliases, and canonical forms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize normalizer with mapping dictionaries.\"\"\"\n",
    "        self.logger = logging.getLogger(\"EntityNormalizer\")\n",
    "        \n",
    "        # Skill normalization map\n",
    "        self.skill_map = {\n",
    "            'js': 'javascript',\n",
    "            'ts': 'typescript',\n",
    "            'py': 'python',\n",
    "            'reactjs': 'react',\n",
    "            'nodejs': 'node.js',\n",
    "            'ml': 'machine learning',\n",
    "            'ai': 'artificial intelligence',\n",
    "            'dl': 'deep learning',\n",
    "            'nlp': 'natural language processing',\n",
    "            'cv': 'computer vision',\n",
    "            'k8s': 'kubernetes',\n",
    "            'tf': 'tensorflow',\n",
    "            'scikit': 'scikit-learn'\n",
    "        }\n",
    "        \n",
    "        # Degree normalization map\n",
    "        self.degree_map = {\n",
    "            'btech': 'B.Tech',\n",
    "            'b.tech': 'B.Tech',\n",
    "            'be': 'B.E',\n",
    "            'b.e': 'B.E',\n",
    "            'mtech': 'M.Tech',\n",
    "            'm.tech': 'M.Tech',\n",
    "            'me': 'M.E',\n",
    "            'm.e': 'M.E',\n",
    "            'bca': 'BCA',\n",
    "            'mca': 'MCA',\n",
    "            'bsc': 'B.Sc',\n",
    "            'b.sc': 'B.Sc',\n",
    "            'msc': 'M.Sc',\n",
    "            'm.sc': 'M.Sc'\n",
    "        }\n",
    "    \n",
    "    def normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize skill to canonical form.\n",
    "        \n",
    "        Args:\n",
    "            skill: Raw skill string\n",
    "            \n",
    "        Returns:\n",
    "            Normalized skill name\n",
    "        \"\"\"\n",
    "        skill_lower = skill.lower().strip()\n",
    "        return self.skill_map.get(skill_lower, skill_lower)\n",
    "    \n",
    "    def normalize_degree(self, degree: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize degree to canonical form.\n",
    "        \n",
    "        Args:\n",
    "            degree: Raw degree string\n",
    "            \n",
    "        Returns:\n",
    "            Normalized degree name\n",
    "        \"\"\"\n",
    "        degree_lower = degree.lower().strip()\n",
    "        return self.degree_map.get(degree_lower, degree.upper())\n",
    "    \n",
    "    def normalize_skills_list(self, skills: List[str]) -> List[str]:\n",
    "        \"\"\"Normalize a list of skills.\"\"\"\n",
    "        normalized = []\n",
    "        seen = set()\n",
    "        for skill in skills:\n",
    "            norm_skill = self.normalize_skill(skill)\n",
    "            if norm_skill not in seen:\n",
    "                normalized.append(norm_skill)\n",
    "                seen.add(norm_skill)\n",
    "        return normalized\n",
    "    \n",
    "    def normalize_degrees_list(self, degrees: List[str]) -> List[str]:\n",
    "        \"\"\"Normalize a list of degrees.\"\"\"\n",
    "        normalized = []\n",
    "        seen = set()\n",
    "        for degree in degrees:\n",
    "            norm_degree = self.normalize_degree(degree)\n",
    "            if norm_degree not in seen:\n",
    "                normalized.append(norm_degree)\n",
    "                seen.add(norm_degree)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# SALARY PARSER\n",
    "# ===================================================================\n",
    "\n",
    "class SalaryParser:\n",
    "    \"\"\"\n",
    "    Parse and normalize salary information from text.\n",
    "    Handles various formats: LPA, monthly, ranges, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize salary parser with regex patterns.\"\"\"\n",
    "        self.logger = logging.getLogger(\"SalaryParser\")\n",
    "        \n",
    "        # Salary patterns\n",
    "        self.patterns = [\n",
    "            # Pattern: \"5-8 LPA\" or \"5 to 8 LPA\"\n",
    "            (r'(\\d+(?:\\.\\d+)?)\\s*(?:-|to)\\s*(\\d+(?:\\.\\d+)?)\\s*(?:lpa|lakhs?\\s+per\\s+annum)', 'lpa_range'),\n",
    "            # Pattern: \"5.5 LPA\"\n",
    "            (r'(\\d+(?:\\.\\d+)?)\\s*(?:lpa|lakhs?\\s+per\\s+annum)', 'lpa_single'),\n",
    "            # Pattern: \"20k per month\" or \"20000 per month\"\n",
    "            (r'(\\d+)k?\\s*(?:per\\s+month|pm|/month)', 'monthly'),\n",
    "            # Pattern: \"CTC: 5 LPA\"\n",
    "            (r'ctc\\s*:?\\s*(\\d+(?:\\.\\d+)?)\\s*(?:lpa|lakhs?)', 'ctc'),\n",
    "            # Pattern: \"Package: 5-8 lakhs\"\n",
    "            (r'package\\s*:?\\s*(\\d+(?:\\.\\d+)?)\\s*(?:-|to)\\s*(\\d+(?:\\.\\d+)?)\\s*lakhs?', 'package_range')\n",
    "        ]\n",
    "    \n",
    "    def parse(self, text: str) -> Optional[Compensation]:\n",
    "        \"\"\"\n",
    "        Parse salary information from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text containing salary info\n",
    "            \n",
    "        Returns:\n",
    "            Compensation object or None\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return None\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        try:\n",
    "            for pattern, pattern_type in self.patterns:\n",
    "                match = re.search(pattern, text_lower, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return self._parse_match(match, pattern_type, text)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing salary '{text}': {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_match(self, match, pattern_type: str, original_text: str) -> Compensation:\n",
    "        \"\"\"Parse regex match into Compensation object.\"\"\"\n",
    "        try:\n",
    "            if pattern_type == 'lpa_range':\n",
    "                min_val = float(match.group(1))\n",
    "                max_val = float(match.group(2))\n",
    "                return Compensation(\n",
    "                    salary_min=int(min_val * 100000),\n",
    "                    salary_max=int(max_val * 100000),\n",
    "                    currency='INR',\n",
    "                    period='annual',\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=0.9\n",
    "                )\n",
    "            \n",
    "            elif pattern_type == 'lpa_single':\n",
    "                val = float(match.group(1))\n",
    "                return Compensation(\n",
    "                    salary_min=int(val * 100000),\n",
    "                    salary_max=int(val * 100000),\n",
    "                    currency='INR',\n",
    "                    period='annual',\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=0.85\n",
    "                )\n",
    "            \n",
    "            elif pattern_type == 'monthly':\n",
    "                val = float(match.group(1))\n",
    "                # Convert to annual\n",
    "                annual = val * 1000 * 12 if val < 1000 else val * 12\n",
    "                return Compensation(\n",
    "                    salary_min=int(annual),\n",
    "                    salary_max=int(annual),\n",
    "                    currency='INR',\n",
    "                    period='annual',\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=0.8\n",
    "                )\n",
    "            \n",
    "            elif pattern_type in ['ctc', 'package_range']:\n",
    "                val = float(match.group(1))\n",
    "                return Compensation(\n",
    "                    salary_min=int(val * 100000),\n",
    "                    salary_max=int(val * 100000),\n",
    "                    currency='INR',\n",
    "                    period='annual',\n",
    "                    raw_text=match.group(0),\n",
    "                    confidence=0.85\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing match: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EXPERIENCE PARSER\n",
    "# ===================================================================\n",
    "\n",
    "class ExperienceParser:\n",
    "    \"\"\"\n",
    "    Parse experience requirements from text.\n",
    "    Handles ranges, single values, and descriptive text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize experience parser.\"\"\"\n",
    "        self.logger = logging.getLogger(\"ExperienceParser\")\n",
    "        \n",
    "        self.patterns = [\n",
    "            # Pattern: \"2-5 years\"\n",
    "            r'(\\d+)\\s*(?:-|to)\\s*(\\d+)\\s*years?',\n",
    "            # Pattern: \"2 years\"\n",
    "            r'(\\d+)\\s*years?\\s+(?:of\\s+)?experience',\n",
    "            # Pattern: \"0-2 years\" (fresher)\n",
    "            r'0\\s*(?:-|to)\\s*(\\d+)\\s*years?'\n",
    "        ]\n",
    "    \n",
    "    def parse(self, text: str) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Parse experience requirement.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (min_years, max_years)\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return (0, 0)\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for fresher keywords\n",
    "        if any(word in text_lower for word in ['fresher', 'freshers', 'entry level']):\n",
    "            return (0, 0)\n",
    "        \n",
    "        try:\n",
    "            for pattern in self.patterns:\n",
    "                match = re.search(pattern, text_lower)\n",
    "                if match:\n",
    "                    groups = match.groups()\n",
    "                    if len(groups) == 2:\n",
    "                        return (int(groups[0]), int(groups[1]))\n",
    "                    elif len(groups) == 1:\n",
    "                        val = int(groups[0])\n",
    "                        return (val, val)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing experience '{text}': {e}\")\n",
    "        \n",
    "        return (0, 0)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# DEADLINE PARSER\n",
    "# ===================================================================\n",
    "\n",
    "class DeadlineParser:\n",
    "    \"\"\"\n",
    "    Parse and normalize deadline dates.\n",
    "    Handles various date formats and relative dates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize deadline parser.\"\"\"\n",
    "        self.logger = logging.getLogger(\"DeadlineParser\")\n",
    "    \n",
    "    def parse(self, text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Parse deadline from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            ISO format date string or None\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Try standard date formats\n",
    "            date_patterns = [\n",
    "                (r'(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})', '%d-%m-%Y'),\n",
    "                (r'(\\d{1,2})[-/](\\d{1,2})[-/](\\d{2})', '%d-%m-%y'),\n",
    "                (r'(\\d{4})[-/](\\d{1,2})[-/](\\d{1,2})', '%Y-%m-%d')\n",
    "            ]\n",
    "            \n",
    "            for pattern, fmt in date_patterns:\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    date_str = match.group(0)\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_str, fmt)\n",
    "                        return dt.strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Handle relative dates\n",
    "            if 'today' in text.lower():\n",
    "                return datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Check for \"in X days\"\n",
    "            match = re.search(r'in\\s+(\\d+)\\s+days?', text.lower())\n",
    "            if match:\n",
    "                days = int(match.group(1))\n",
    "                future_date = datetime.now() + timedelta(days=days)\n",
    "                return future_date.strftime('%Y-%m-%d')\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing deadline '{text}': {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# RELATIONSHIP EXTRACTOR\n",
    "# ===================================================================\n",
    "\n",
    "class RelationshipExtractor:\n",
    "    \"\"\"\n",
    "    Extract relationships between entities.\n",
    "    Links companies with positions, skills, locations, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize relationship extractor.\"\"\"\n",
    "        self.logger = logging.getLogger(\"RelationshipExtractor\")\n",
    "        self.normalizer = EntityNormalizer()\n",
    "        self.salary_parser = SalaryParser()\n",
    "        self.experience_parser = ExperienceParser()\n",
    "        self.deadline_parser = DeadlineParser()\n",
    "    \n",
    "    def extract_job_postings(\n",
    "        self,\n",
    "        row: pd.Series,\n",
    "        email_id: str\n",
    "    ) -> List[JobPosting]:\n",
    "        \"\"\"\n",
    "        Extract structured job postings from a DataFrame row.\n",
    "        \n",
    "        Args:\n",
    "            row: DataFrame row with extracted entities\n",
    "            email_id: Unique email identifier\n",
    "            \n",
    "        Returns:\n",
    "            List of structured JobPosting objects\n",
    "        \"\"\"\n",
    "        job_postings = []\n",
    "        \n",
    "        try:\n",
    "            # Parse entities from row\n",
    "            companies = self._parse_list(row.get('companies_extracted', ''))\n",
    "            positions = self._parse_list(row.get('positions_extracted', ''))\n",
    "            skills = self._parse_list(row.get('skills_extracted', ''))\n",
    "            locations = self._parse_list(row.get('locations_extracted', ''))\n",
    "            salary_texts = self._parse_list(row.get('salary_info', ''))\n",
    "            experience_texts = self._parse_list(row.get('experience_required', ''))\n",
    "            degrees = self._parse_list(row.get('degrees_required', ''))\n",
    "            \n",
    "            # If no companies or positions, return empty\n",
    "            if not companies or not positions:\n",
    "                self.logger.debug(f\"No companies or positions found in {email_id}\")\n",
    "                return []\n",
    "            \n",
    "            # Normalize entities\n",
    "            skills = self.normalizer.normalize_skills_list(skills)\n",
    "            degrees = self.normalizer.normalize_degrees_list(degrees)\n",
    "            \n",
    "            # Create job postings\n",
    "            # Strategy: Create one posting per (company, position) pair\n",
    "            for idx, (company_name, position_title) in enumerate(\n",
    "                self._create_pairs(companies, positions)\n",
    "            ):\n",
    "                job_id = f\"{email_id}_JOB_{idx+1}\"\n",
    "                \n",
    "                try:\n",
    "                    # Create structured entities\n",
    "                    company = Company(\n",
    "                        name=company_name,\n",
    "                        canonical_name='',\n",
    "                        confidence=0.85\n",
    "                    )\n",
    "                    \n",
    "                    position = Position(\n",
    "                        title=position_title,\n",
    "                        confidence=0.85\n",
    "                    )\n",
    "                    \n",
    "                    # Parse experience\n",
    "                    exp_min, exp_max = 0, 0\n",
    "                    if experience_texts:\n",
    "                        exp_min, exp_max = self.experience_parser.parse(\n",
    "                            experience_texts[0]\n",
    "                        )\n",
    "                    \n",
    "                    requirements = Requirements(\n",
    "                        skills=skills,\n",
    "                        education=degrees,\n",
    "                        experience_min=exp_min,\n",
    "                        experience_max=exp_max\n",
    "                    )\n",
    "                    \n",
    "                    # Parse location\n",
    "                    location_city = locations[idx] if idx < len(locations) else (\n",
    "                        locations[0] if locations else \"Not Specified\"\n",
    "                    )\n",
    "                    \n",
    "                    location = Location(\n",
    "                        city=location_city,\n",
    "                        confidence=0.8\n",
    "                    )\n",
    "                    \n",
    "                    # Parse salary\n",
    "                    compensation = Compensation()\n",
    "                    if salary_texts:\n",
    "                        parsed_salary = self.salary_parser.parse(salary_texts[0])\n",
    "                        if parsed_salary:\n",
    "                            compensation = parsed_salary\n",
    "                    \n",
    "                    # Parse deadline\n",
    "                    deadline = None\n",
    "                    deadline_col = row.get('deadline', '')\n",
    "                    if deadline_col:\n",
    "                        deadline = self.deadline_parser.parse(str(deadline_col))\n",
    "                    \n",
    "                    application = Application(\n",
    "                        deadline=deadline\n",
    "                    )\n",
    "                    \n",
    "                    # Create job posting\n",
    "                    job_posting = JobPosting(\n",
    "                        job_id=job_id,\n",
    "                        email_id=email_id,\n",
    "                        company=company,\n",
    "                        position=position,\n",
    "                        requirements=requirements,\n",
    "                        location=location,\n",
    "                        compensation=compensation,\n",
    "                        application=application,\n",
    "                        metadata={\n",
    "                            'extraction_timestamp': datetime.now().isoformat(),\n",
    "                            'completeness_score': 0.0,\n",
    "                            'source_subject': str(row.get('Subject', ''))[:100]\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate completeness\n",
    "                    job_posting.metadata['completeness_score'] = (\n",
    "                        job_posting.calculate_completeness()\n",
    "                    )\n",
    "                    \n",
    "                    job_postings.append(job_posting)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(\n",
    "                        f\"Error creating job posting {job_id}: {e}\"\n",
    "                    )\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error extracting job postings from {email_id}: {e}\"\n",
    "            )\n",
    "        \n",
    "        return job_postings\n",
    "    \n",
    "    def _parse_list(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse comma-separated string into list.\"\"\"\n",
    "        if not text or pd.isna(text) or text == '':\n",
    "            return []\n",
    "        return [item.strip() for item in str(text).split(',') if item.strip()]\n",
    "    \n",
    "    def _create_pairs(\n",
    "        self,\n",
    "        companies: List[str],\n",
    "        positions: List[str]\n",
    "    ) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Create company-position pairs.\n",
    "        \n",
    "        Strategy:\n",
    "        - If counts match: pair 1-to-1\n",
    "        - If different: create all combinations (max 5)\n",
    "        \"\"\"\n",
    "        if len(companies) == len(positions):\n",
    "            return list(zip(companies, positions))\n",
    "        \n",
    "        # Create combinations but limit to avoid explosion\n",
    "        pairs = []\n",
    "        for company in companies[:3]:  # Max 3 companies\n",
    "            for position in positions[:3]:  # Max 3 positions\n",
    "                pairs.append((company, position))\n",
    "        \n",
    "        return pairs[:5]  # Max 5 job postings per email\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN STRUCTURING PIPELINE\n",
    "# ===================================================================\n",
    "\n",
    "class EntityStructuringPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline for entity structuring and relationship extraction.\n",
    "    Orchestrates all components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize pipeline.\"\"\"\n",
    "        self.logger = logging.getLogger(\"StructuringPipeline\")\n",
    "        self.relationship_extractor = RelationshipExtractor()\n",
    "        \n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(\"ðŸ—ï¸  ENTITY STRUCTURING PIPELINE INITIALIZED\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "    \n",
    "    def process_dataset(\n",
    "        self,\n",
    "        input_csv: str,\n",
    "        output_csv: str = \"structured_job_postings.csv\",\n",
    "        output_json: str = \"structured_job_postings.json\"\n",
    "    ) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Process entire dataset and create structured job postings.\n",
    "        \n",
    "        Args:\n",
    "            input_csv: Input CSV file path\n",
    "            output_csv: Output CSV file path\n",
    "            output_json: Output JSON file path\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (DataFrame, JSON list)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\nLoading dataset: {input_csv}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(input_csv)\n",
    "            total_emails = len(df)\n",
    "            self.logger.info(f\"Loaded {total_emails} emails\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Process each email\n",
    "        self.logger.info(f\"\\nProcessing {total_emails} emails...\")\n",
    "        \n",
    "        all_job_postings = []\n",
    "        stats = {\n",
    "            'total_emails': total_emails,\n",
    "            'emails_with_jobs': 0,\n",
    "            'total_jobs': 0,\n",
    "            'avg_completeness': 0.0\n",
    "        }\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            email_id = row.get('MessageId', f\"EMAIL_{idx}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract job postings from this email\n",
    "                job_postings = self.relationship_extractor.extract_job_postings(\n",
    "                    row, email_id\n",
    "                )\n",
    "                \n",
    "                if job_postings:\n",
    "                    stats['emails_with_jobs'] += 1\n",
    "                    stats['total_jobs'] += len(job_postings)\n",
    "                    \n",
    "                    for job in job_postings:\n",
    "                        all_job_postings.append(job.to_dict())\n",
    "                \n",
    "                # Log progress every 50 emails\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    self.logger.info(\n",
    "                        f\"   Processed {idx+1}/{total_emails} emails | \"\n",
    "                        f\"Jobs extracted: {stats['total_jobs']}\"\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing email {email_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if all_job_postings:\n",
    "            completeness_scores = [\n",
    "                job['metadata']['completeness_score'] \n",
    "                for job in all_job_postings\n",
    "            ]\n",
    "            stats['avg_completeness'] = np.mean(completeness_scores)\n",
    "        \n",
    "        # Log results\n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"PROCESSING COMPLETE\")\n",
    "        self.logger.info(f\"{'='*70}\")\n",
    "        self.logger.info(f\"Total Emails Processed: {stats['total_emails']}\")\n",
    "        self.logger.info(\n",
    "            f\"Emails with Job Postings: {stats['emails_with_jobs']} \"\n",
    "            f\"({stats['emails_with_jobs']/stats['total_emails']*100:.1f}%)\"\n",
    "        )\n",
    "        self.logger.info(f\"Total Job Postings Created: {stats['total_jobs']}\")\n",
    "        self.logger.info(\n",
    "            f\"Average Completeness Score: {stats['avg_completeness']:.2%}\"\n",
    "        )\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Save outputs\n",
    "        self._save_outputs(all_job_postings, output_csv, output_json)\n",
    "        \n",
    "        # Create DataFrame for easy querying\n",
    "        jobs_df = self._create_dataframe(all_job_postings)\n",
    "        \n",
    "        return jobs_df, all_job_postings\n",
    "    \n",
    "    def _save_outputs(\n",
    "        self,\n",
    "        job_postings: List[Dict],\n",
    "        csv_path: str,\n",
    "        json_path: str\n",
    "    ):\n",
    "        \"\"\"Save structured job postings to CSV and JSON.\"\"\"\n",
    "        try:\n",
    "            # Save JSON (complete structure)\n",
    "            self.logger.info(f\"Saving JSON: {json_path}\")\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(job_postings, f, indent=2, ensure_ascii=False)\n",
    "            self.logger.info(f\"Saved {len(job_postings)} jobs to JSON\")\n",
    "            \n",
    "            # Save CSV (flattened structure)\n",
    "            self.logger.info(f\"Saving CSV: {csv_path}\")\n",
    "            df = self._create_dataframe(job_postings)\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            self.logger.info(f\"Saved {len(df)} jobs to CSV\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving outputs: {e}\")\n",
    "    \n",
    "    def _create_dataframe(self, job_postings: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create flattened DataFrame from structured job postings.\n",
    "        \n",
    "        Args:\n",
    "            job_postings: List of job posting dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Flattened pandas DataFrame\n",
    "        \"\"\"\n",
    "        flattened = []\n",
    "        \n",
    "        for job in job_postings:\n",
    "            try:\n",
    "                flat_job = {\n",
    "                    # IDs\n",
    "                    'job_id': job['job_id'],\n",
    "                    'email_id': job['email_id'],\n",
    "                    \n",
    "                    # Company\n",
    "                    'company_name': job['company']['name'],\n",
    "                    'company_canonical': job['company']['canonical_name'],\n",
    "                    'company_confidence': job['company']['confidence'],\n",
    "                    \n",
    "                    # Position\n",
    "                    'position_title': job['position']['title'],\n",
    "                    'position_level': job['position']['level'],\n",
    "                    'position_confidence': job['position']['confidence'],\n",
    "                    \n",
    "                    # Requirements\n",
    "                    'skills_required': ', '.join(job['requirements']['skills']),\n",
    "                    'skills_count': len(job['requirements']['skills']),\n",
    "                    'education_required': ', '.join(job['requirements']['education']),\n",
    "                    'experience_min_years': job['requirements']['experience_min'],\n",
    "                    'experience_max_years': job['requirements']['experience_max'],\n",
    "                    'experience_type': job['requirements']['experience_type'],\n",
    "                    \n",
    "                    # Location\n",
    "                    'location_city': job['location']['city'],\n",
    "                    'location_state': job['location']['state'],\n",
    "                    'work_mode': job['location']['work_mode'],\n",
    "                    'location_confidence': job['location']['confidence'],\n",
    "                    \n",
    "                    # Compensation\n",
    "                    'salary_min': job['compensation']['salary_min'],\n",
    "                    'salary_max': job['compensation']['salary_max'],\n",
    "                    'salary_currency': job['compensation']['currency'],\n",
    "                    'salary_period': job['compensation']['period'],\n",
    "                    'salary_raw_text': job['compensation']['raw_text'],\n",
    "                    'salary_confidence': job['compensation']['confidence'],\n",
    "                    \n",
    "                    # Application\n",
    "                    'application_deadline': job['application']['deadline'],\n",
    "                    'apply_link': job['application']['apply_link'],\n",
    "                    'contact_email': job['application']['contact_email'],\n",
    "                    \n",
    "                    # Metadata\n",
    "                    'completeness_score': job['metadata']['completeness_score'],\n",
    "                    'extraction_timestamp': job['metadata']['extraction_timestamp'],\n",
    "                    'source_subject': job['metadata']['source_subject']\n",
    "                }\n",
    "                \n",
    "                flattened.append(flat_job)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error flattening job {job.get('job_id')}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(flattened)\n",
    "    \n",
    "    def generate_analytics(\n",
    "        self,\n",
    "        csv_path: str = \"structured_job_postings.csv\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate analytics report on structured data.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to structured CSV file\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"ANALYTICS REPORT\")\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Basic statistics\n",
    "            self.logger.info(f\"BASIC STATISTICS:\")\n",
    "            self.logger.info(f\"   Total Job Postings: {len(df)}\")\n",
    "            self.logger.info(f\"   Unique Companies: {df['company_name'].nunique()}\")\n",
    "            self.logger.info(f\"   Unique Positions: {df['position_title'].nunique()}\")\n",
    "            self.logger.info(f\"   Unique Locations: {df['location_city'].nunique()}\")\n",
    "            \n",
    "            # Top companies\n",
    "            self.logger.info(f\"\\nTOP 15 HIRING COMPANIES:\")\n",
    "            top_companies = df['company_name'].value_counts().head(15)\n",
    "            for company, count in top_companies.items():\n",
    "                self.logger.info(f\"   {company}: {count} positions\")\n",
    "            \n",
    "            # Top positions\n",
    "            self.logger.info(f\"\\nTOP 15 JOB POSITIONS:\")\n",
    "            top_positions = df['position_title'].value_counts().head(15)\n",
    "            for position, count in top_positions.items():\n",
    "                self.logger.info(f\"   {position}: {count} openings\")\n",
    "            \n",
    "            # Top locations\n",
    "            self.logger.info(f\"\\nTOP 10 LOCATIONS:\")\n",
    "            top_locations = df['location_city'].value_counts().head(10)\n",
    "            for location, count in top_locations.items():\n",
    "                self.logger.info(f\"   {location}: {count} jobs\")\n",
    "            \n",
    "            # Top skills\n",
    "            all_skills = []\n",
    "            for skills_str in df['skills_required']:\n",
    "                if pd.notna(skills_str) and skills_str:\n",
    "                    all_skills.extend([s.strip() for s in skills_str.split(',')])\n",
    "            \n",
    "            if all_skills:\n",
    "                skill_counts = Counter(all_skills)\n",
    "                self.logger.info(f\"\\nTOP 20 MOST DEMANDED SKILLS:\")\n",
    "                for skill, count in skill_counts.most_common(20):\n",
    "                    self.logger.info(f\"   {skill}: {count} mentions\")\n",
    "            \n",
    "            # Salary statistics\n",
    "            salary_data = df[df['salary_max'] > 0]['salary_max']\n",
    "            if len(salary_data) > 0:\n",
    "                self.logger.info(f\"\\nSALARY STATISTICS:\")\n",
    "                self.logger.info(\n",
    "                    f\"   Jobs with Salary Info: {len(salary_data)} \"\n",
    "                    f\"({len(salary_data)/len(df)*100:.1f}%)\"\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"   Average Salary: â‚¹{salary_data.mean()/100000:.2f} LPA\"\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"   Median Salary: â‚¹{salary_data.median()/100000:.2f} LPA\"\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"   Min Salary: â‚¹{salary_data.min()/100000:.2f} LPA\"\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"   Max Salary: â‚¹{salary_data.max()/100000:.2f} LPA\"\n",
    "                )\n",
    "            \n",
    "            # Experience distribution\n",
    "            self.logger.info(f\"\\nEXPERIENCE LEVEL DISTRIBUTION:\")\n",
    "            exp_dist = df['experience_type'].value_counts()\n",
    "            for exp_type, count in exp_dist.items():\n",
    "                self.logger.info(\n",
    "                    f\"   {exp_type}: {count} ({count/len(df)*100:.1f}%)\"\n",
    "                )\n",
    "            \n",
    "            # Work mode distribution\n",
    "            self.logger.info(f\"\\nWORK MODE DISTRIBUTION:\")\n",
    "            work_mode_dist = df['work_mode'].value_counts()\n",
    "            for mode, count in work_mode_dist.items():\n",
    "                self.logger.info(\n",
    "                    f\"   {mode}: {count} ({count/len(df)*100:.1f}%)\"\n",
    "                )\n",
    "            \n",
    "            # Completeness distribution\n",
    "            self.logger.info(f\"\\nDATA COMPLETENESS:\")\n",
    "            avg_completeness = df['completeness_score'].mean()\n",
    "            self.logger.info(f\"   Average Completeness: {avg_completeness:.2%}\")\n",
    "            \n",
    "            high_quality = len(df[df['completeness_score'] >= 0.7])\n",
    "            medium_quality = len(\n",
    "                df[(df['completeness_score'] >= 0.5) & \n",
    "                   (df['completeness_score'] < 0.7)]\n",
    "            )\n",
    "            low_quality = len(df[df['completeness_score'] < 0.5])\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"   High Quality (â‰¥70%): {high_quality} \"\n",
    "                f\"({high_quality/len(df)*100:.1f}%)\"\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f\"   Medium Quality (50-70%): {medium_quality} \"\n",
    "                f\"({medium_quality/len(df)*100:.1f}%)\"\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f\"   Low Quality (<50%): {low_quality} \"\n",
    "                f\"({low_quality/len(df)*100:.1f}%)\"\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"\\n{'='*70}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating analytics: {e}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"STARTING ENTITY STRUCTURING PIPELINE\")\n",
    "    logger.info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize pipeline\n",
    "        pipeline = EntityStructuringPipeline()\n",
    "        \n",
    "        # Process dataset\n",
    "        input_path = r\"D:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Phase_scripts\\Phase 2\\relevant_placement_emails.csv\"\n",
    "        input_file = input_path\n",
    "        output_csv = \"structured_job_postings.csv\"\n",
    "        output_json = \"structured_job_postings.json\"\n",
    "        \n",
    "        jobs_df, jobs_list = pipeline.process_dataset(\n",
    "            input_csv=input_file,\n",
    "            output_csv=output_csv,\n",
    "            output_json=output_json\n",
    "        )\n",
    "        \n",
    "        # Generate analytics\n",
    "        pipeline.generate_analytics(output_csv)\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Output Files:\")\n",
    "        logger.info(f\"   - CSV: {output_csv}\")\n",
    "        logger.info(f\"   - JSON: {output_json}\")\n",
    "        logger.info(f\"   - Log: entity_structuring.log\")\n",
    "        logger.info(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return jobs_df, jobs_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"PIPELINE FAILED: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute pipeline\n",
    "    jobs_df, jobs_json = main()\n",
    "    \n",
    "    # Optional: Display sample\n",
    "    print(\"\\nSAMPLE STRUCTURED JOB POSTING:\")\n",
    "    print(\"=\"*70)\n",
    "    if len(jobs_df) > 0:\n",
    "        sample = jobs_df.iloc[0].to_dict()\n",
    "        for key, value in sample.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
