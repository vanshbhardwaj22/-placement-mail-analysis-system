{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_and_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# AI-POWERED DATA CLEANING PIPELINE FOR PLACEMENT EMAILS\n",
    "# Using: spaCy Transformer + NER + Pattern Matching + Config Integration\n",
    "# ===================================================================\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, json, re, time, logging, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Any, Optional\n",
    "\n",
    "# Import incremental state management\n",
    "from incremental_state_management import (\n",
    "    load_processed_ids, save_processed_ids,\n",
    "    save_checkpoint, load_checkpoint, clear_checkpoint\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"AI_Cleaning\")\n",
    "\n",
    "# Try importing spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher, PhraseMatcher\n",
    "    from spacy.tokens import Span\n",
    "    SPACY_AVAILABLE = True\n",
    "    logger.info(\"spaCy available\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    logger.warning(\"spaCy not available - install: pip install spacy\")\n",
    "\n",
    "logger.info(\"AI-Powered Data Cleaning Pipeline with Incremental Processing\")\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path: str = \"config.json\") -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        logger.info(f\"Configuration loaded from: {config_path}\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Config file not found: {config_path}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON in config file: {e}\")\n",
    "        raise\n",
    "\n",
    "CONFIG = load_config()\n",
    "INPUT_CSV = CONFIG.get('input_csv', '../Phase 1/placement_emails.csv')\n",
    "OUTPUT_CSV = CONFIG.get('output_csv', 'ai_cleaned_emails.csv')\n",
    "KNOWLEDGE_BASE = CONFIG.get('knowledge_base', {})\n",
    "EXTRACTION_PATTERNS = CONFIG.get('extraction_patterns', {})\n",
    "TEXT_CLEANING_CONFIG = CONFIG.get('text_cleaning', {})\n",
    "\n",
    "# Incremental processing configuration\n",
    "INCREMENTAL_CONFIG = CONFIG.get('incremental_processing', {})\n",
    "INCREMENTAL_ENABLED = INCREMENTAL_CONFIG.get('enabled', True)\n",
    "STATE_DIR = INCREMENTAL_CONFIG.get('state_directory', 'state')\n",
    "STATE_FILE = os.path.join(STATE_DIR, INCREMENTAL_CONFIG.get('state_file', 'cleaned_message_ids.txt'))\n",
    "FORCE_FULL_REPROCESS = INCREMENTAL_CONFIG.get('force_full_reprocess', False)\n",
    "\n",
    "logger.info(f\"Input: {INPUT_CSV} | Output: {OUTPUT_CSV}\")\n",
    "logger.info(f\"Incremental Mode: {'ENABLED' if INCREMENTAL_ENABLED else 'DISABLED'}\")\n",
    "if FORCE_FULL_REPROCESS:\n",
    "    logger.warning(\"FORCE FULL REPROCESS MODE - Will process all emails\")\n",
    "logger.info(f\"Knowledge Base: {len(KNOWLEDGE_BASE.get('skills', []))} skills, {len(KNOWLEDGE_BASE.get('positions', []))} positions\")\n",
    "\n",
    "# Load spaCy model\n",
    "def load_nlp_model():\n",
    "    if not SPACY_AVAILABLE:\n",
    "        return None\n",
    "    for model_name in [\"en_core_web_trf\", \"en_core_web_lg\", \"en_core_web_md\", \"en_core_web_sm\"]:\n",
    "        try:\n",
    "            nlp = spacy.load(model_name)\n",
    "            logger.info(f\"Loaded spaCy model: {model_name}\")\n",
    "            return nlp\n",
    "        except:\n",
    "            continue\n",
    "    logger.error(\"No spaCy model found! Install with: python -m spacy download en_core_web_sm\")\n",
    "    return None\n",
    "\n",
    "NLP_MODEL = load_nlp_model()\n",
    "\n",
    "# Convert config to sets for fast lookup\n",
    "SKILLS_SET = set(skill.lower() for skill in KNOWLEDGE_BASE.get('skills', []))\n",
    "POSITIONS_SET = set(pos.lower() for pos in KNOWLEDGE_BASE.get('positions', []))\n",
    "LOCATIONS_SET = set(loc.lower() for loc in KNOWLEDGE_BASE.get('locations', []))\n",
    "DEGREES_SET = set(deg.lower() for deg in KNOWLEDGE_BASE.get('degrees', []))\n",
    "\n",
    "# Compile regex patterns\n",
    "COMPANY_PATTERNS = [re.compile(p) for p in EXTRACTION_PATTERNS.get('company_patterns', [])]\n",
    "SALARY_PATTERNS = [re.compile(p, re.IGNORECASE) for p in EXTRACTION_PATTERNS.get('salary_patterns', [])]\n",
    "EXPERIENCE_PATTERNS = [re.compile(p, re.IGNORECASE) for p in EXTRACTION_PATTERNS.get('experience_patterns', [])]\n",
    "REMOVE_PATTERNS = [re.compile(p, re.IGNORECASE | re.DOTALL) for p in TEXT_CLEANING_CONFIG.get('remove_patterns', [])]\n",
    "MIN_LINE_LENGTH = TEXT_CLEANING_CONFIG.get('min_line_length', 5)\n",
    "\n",
    "logger.info(f\"Compiled {len(COMPANY_PATTERNS)} company, {len(SALARY_PATTERNS)} salary, {len(EXPERIENCE_PATTERNS)} experience patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text_cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning functions\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "PHONE_RE = re.compile(r'[\\+]?[\\d][\\d\\s\\-\\(\\)]{7,}[\\d]')\n",
    "HTML_RE = re.compile(r'<[^>]+>')\n",
    "NON_PRINTABLE = re.compile(r'[^\\x20-\\x7E\\n\\r]+')\n",
    "\n",
    "def clean_text_ai(text: str) -> str:\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = EMAIL_RE.sub(\" \", text)\n",
    "    text = PHONE_RE.sub(\" \", text)\n",
    "    for pattern in REMOVE_PATTERNS:\n",
    "        text = pattern.sub(\"\", text)\n",
    "    text = NON_PRINTABLE.sub(\" \", text)\n",
    "    text = re.sub(r'\\n\\s*\\n+', \"\\n\", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > MIN_LINE_LENGTH]\n",
    "    return \" \".join(lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entity_extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-powered entity extraction\n",
    "def extract_with_ai(text: str, nlp_model) -> Dict[str, Any]:\n",
    "    result = {'companies': set(), 'skills': set(), 'positions': set(), 'locations': set(),\n",
    "              'salary_info': [], 'experience_required': [], 'degrees_required': set()}\n",
    "    if not nlp_model or not text:\n",
    "        return result\n",
    "    try:\n",
    "        doc = nlp_model(text[:5000])\n",
    "        text_lower = text.lower()\n",
    "        # Extract organizations\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                org = ent.text.strip()\n",
    "                if len(org) > 2 and org.lower() not in ['we', 'our', 'the', 'a', 'an']:\n",
    "                    result['companies'].add(org)\n",
    "        for pattern in COMPANY_PATTERNS:\n",
    "            for match in pattern.finditer(text):\n",
    "                if len(match.group(0).strip()) > 3:\n",
    "                    result['companies'].add(match.group(0).strip())\n",
    "        # Extract skills\n",
    "        for token in doc:\n",
    "            if token.text.lower() in SKILLS_SET or token.lemma_.lower() in SKILLS_SET:\n",
    "                result['skills'].add(token.text.lower())\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.text.lower() in SKILLS_SET:\n",
    "                result['skills'].add(chunk.text.lower())\n",
    "        # Extract positions\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.text.lower() in POSITIONS_SET:\n",
    "                result['positions'].add(chunk.text.lower())\n",
    "        for token in doc:\n",
    "            if token.text.lower() in POSITIONS_SET:\n",
    "                result['positions'].add(token.text.lower())\n",
    "        # Extract locations\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\" and ent.text.lower().strip() in LOCATIONS_SET:\n",
    "                result['locations'].add(ent.text.lower().strip())\n",
    "        for location in LOCATIONS_SET:\n",
    "            if location in text_lower:\n",
    "                result['locations'].add(location)\n",
    "        # Extract salary and experience\n",
    "        for pattern in SALARY_PATTERNS:\n",
    "            result['salary_info'].extend([m.group(0).strip() for m in pattern.finditer(text)])\n",
    "        for pattern in EXPERIENCE_PATTERNS:\n",
    "            result['experience_required'].extend([m.group(0).strip() for m in pattern.finditer(text)])\n",
    "        # Extract degrees\n",
    "        for degree in DEGREES_SET:\n",
    "            if re.search(r'\\b' + re.escape(degree) + r'\\b', text_lower):\n",
    "                result['degrees_required'].add(degree)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction error: {str(e)[:100]}\")\n",
    "    # Convert to sorted lists\n",
    "    for key in ['companies', 'skills', 'positions', 'locations', 'degrees_required']:\n",
    "        result[key] = sorted(list(result[key]))\n",
    "    result['salary_info'] = sorted(list(set(result['salary_info'])))\n",
    "    result['experience_required'] = sorted(list(set(result['experience_required'])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email processing\n",
    "def process_email_ai(text: str, nlp_model) -> Dict[str, Any]:\n",
    "    result = {'cleaned_text': '', 'companies': [], 'skills': [], 'positions': [], 'locations': [],\n",
    "              'salary_info': [], 'experience_required': [], 'degrees_required': [],\n",
    "              'word_count': 0, 'char_count': 0, 'processing_status': 'success'}\n",
    "    try:\n",
    "        cleaned = clean_text_ai(text)\n",
    "        if not cleaned or len(cleaned) < 10:\n",
    "            result['processing_status'] = 'empty_after_cleaning'\n",
    "            return result\n",
    "        result['cleaned_text'] = cleaned\n",
    "        result['word_count'] = len(cleaned.split())\n",
    "        result['char_count'] = len(cleaned)\n",
    "        if nlp_model:\n",
    "            result.update(extract_with_ai(cleaned, nlp_model))\n",
    "        else:\n",
    "            result['processing_status'] = 'no_ai_model'\n",
    "    except Exception as e:\n",
    "        result['processing_status'] = f'error: {str(e)[:100]}'\n",
    "        logger.error(f\"Processing error: {str(e)}\")\n",
    "    return result\n",
    "\n",
    "# Batch processing with checkpoints\n",
    "def process_batch_ai(df_to_process: pd.DataFrame, nlp_model, batch_size: int = 20, checkpoint_interval: int = 50) -> pd.DataFrame:\n",
    "    total = len(df_to_process)\n",
    "    logger.info(f\"\\nProcessing {total} NEW emails in batches of {batch_size}...\")\n",
    "    all_results = []\n",
    "    processed_ids = set()\n",
    "    stats = {'total': 0, 'with_companies': 0, 'with_skills': 0, 'with_positions': 0, 'with_locations': 0, 'with_salary': 0, 'empty': 0}\n",
    "    start_time = time.time()\n",
    "    for idx, row in df_to_process.iterrows():\n",
    "        try:\n",
    "            text = str(row.get('Subject', '')) + ' ' + str(row.get('Preview', '')) + ' ' + str(row.get('Body', ''))\n",
    "            result = process_email_ai(text, nlp_model)\n",
    "            \n",
    "            # Add original columns\n",
    "            result_row = row.to_dict()\n",
    "            result_row.update({\n",
    "                'cleaned_text': result['cleaned_text'],\n",
    "                'word_count': result['word_count'],\n",
    "                'char_count': result['char_count'],\n",
    "                'processing_status': result['processing_status'],\n",
    "                'companies_extracted': ', '.join(result['companies']),\n",
    "                'skills_extracted': ', '.join(result['skills']),\n",
    "                'positions_extracted': ', '.join(result['positions']),\n",
    "                'locations_extracted': ', '.join(result['locations']),\n",
    "                'salary_info': ', '.join(result['salary_info']),\n",
    "                'experience_required': ', '.join(result['experience_required']),\n",
    "                'degrees_required': ', '.join(result['degrees_required']),\n",
    "                'company_count': len(result['companies']),\n",
    "                'skill_count': len(result['skills']),\n",
    "                'position_count': len(result['positions']),\n",
    "                'location_count': len(result['locations'])\n",
    "            })\n",
    "            \n",
    "            all_results.append(result_row)\n",
    "            processed_ids.add(row['MessageId'])\n",
    "            \n",
    "            # Update stats\n",
    "            stats['total'] += 1\n",
    "            if result['companies']: stats['with_companies'] += 1\n",
    "            if result['skills']: stats['with_skills'] += 1\n",
    "            if result['positions']: stats['with_positions'] += 1\n",
    "            if result['locations']: stats['with_locations'] += 1\n",
    "            if result['salary_info']: stats['with_salary'] += 1\n",
    "            if result['processing_status'] == 'empty_after_cleaning': stats['empty'] += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if stats['total'] % checkpoint_interval == 0:\n",
    "                save_checkpoint(STATE_DIR, processed_ids)\n",
    "                logger.info(f\"Processed {stats['total']}/{total} | Checkpoint saved\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing email {row.get('MessageId', 'unknown')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"\\nProcessing complete in {elapsed:.2f}s ({total/elapsed:.1f} emails/sec)\")\n",
    "    logger.info(f\"Stats: {stats['with_companies']} companies, {stats['with_skills']} skills, {stats['with_positions']} positions\")\n",
    "    \n",
    "    return pd.DataFrame(all_results), processed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline with incremental processing\n",
    "def main_ai_pipeline_incremental(csv_path: str = None, output_path: str = None):\n",
    "    csv_path = csv_path or INPUT_CSV\n",
    "    output_path = output_path or OUTPUT_CSV\n",
    "    \n",
    "    if not NLP_MODEL:\n",
    "        logger.error(\"Cannot proceed without NLP model!\")\n",
    "        return None\n",
    "    \n",
    "    # Load all emails from Phase 1\n",
    "    logger.info(f\"Loading dataset: {csv_path}\")\n",
    "    all_emails_df = pd.read_csv(csv_path)\n",
    "    logger.info(f\"Loaded {len(all_emails_df)} total emails from Phase 1\")\n",
    "    \n",
    "    # Load existing cleaned data\n",
    "    existing_df = pd.DataFrame()\n",
    "    if os.path.exists(output_path) and INCREMENTAL_ENABLED and not FORCE_FULL_REPROCESS:\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        logger.info(f\"Loaded {len(existing_df)} existing cleaned emails\")\n",
    "    \n",
    "    # Determine which emails to process\n",
    "    if INCREMENTAL_ENABLED and not FORCE_FULL_REPROCESS:\n",
    "        # Load processed IDs from state\n",
    "        processed_ids = load_processed_ids(STATE_FILE)\n",
    "        \n",
    "        # Also check checkpoint for crash recovery\n",
    "        checkpoint_ids = load_checkpoint(STATE_DIR)\n",
    "        processed_ids = processed_ids.union(checkpoint_ids)\n",
    "        \n",
    "        # Filter out already processed emails\n",
    "        new_emails_df = all_emails_df[~all_emails_df['MessageId'].isin(processed_ids)]\n",
    "        \n",
    "        logger.info(f\"Already processed: {len(processed_ids)} emails\")\n",
    "        logger.info(f\"New emails to process: {len(new_emails_df)}\")\n",
    "        \n",
    "        if len(new_emails_df) == 0:\n",
    "            logger.info(\"✅ No new emails to process!\")\n",
    "            return existing_df\n",
    "    else:\n",
    "        new_emails_df = all_emails_df\n",
    "        logger.info(f\"Full reprocessing mode: Processing all {len(new_emails_df)} emails\")\n",
    "    \n",
    "    # Process new emails\n",
    "    new_results_df, new_processed_ids = process_batch_ai(new_emails_df, NLP_MODEL, batch_size=20)\n",
    "    \n",
    "    # Save processed IDs to state\n",
    "    if INCREMENTAL_ENABLED:\n",
    "        save_processed_ids(STATE_FILE, new_processed_ids)\n",
    "        clear_checkpoint(STATE_DIR)\n",
    "    \n",
    "    # Merge with existing data\n",
    "    if not existing_df.empty and INCREMENTAL_ENABLED and not FORCE_FULL_REPROCESS:\n",
    "        logger.info(\"Merging with existing cleaned data...\")\n",
    "        combined_df = pd.concat([existing_df, new_results_df], ignore_index=True)\n",
    "        logger.info(f\"Total emails after merge: {len(combined_df)}\")\n",
    "    else:\n",
    "        combined_df = new_results_df\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"✅ Saved {len(combined_df)} total cleaned emails to: {output_path}\")\n",
    "    logger.info(f\"   ({len(new_results_df)} newly processed in this run)\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the incremental pipeline\n",
    "df = main_ai_pipeline_incremental()\n",
    "\n",
    "if df is not None:\n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"PIPELINE COMPLETE!\")\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(f\"Total cleaned emails: {len(df)}\")\n",
    "    logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nSample output:\")\n",
    "    display(df[['Subject', 'companies_extracted', 'skills_extracted', 'positions_extracted']].head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
