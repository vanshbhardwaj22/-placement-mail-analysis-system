{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d62424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# AI-POWERED DATA CLEANING PIPELINE FOR PLACEMENT EMAILS\n",
    "# Using: spaCy Transformer + NER + Pattern Matching\n",
    "# Much Better Accuracy Than Rule-Based Approach\n",
    "# ===================================================================\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, json, re, time, logging, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Any, Optional\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# ===================================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# ===================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"AI_Cleaning\")\n",
    "\n",
    "# Try importing advanced NLP libraries\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher, PhraseMatcher\n",
    "    from spacy.tokens import Span\n",
    "    SPACY_AVAILABLE = True\n",
    "    logger.info(\"spaCy available\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    logger.warning(\"spaCy not available - install: pip install spacy\")\n",
    "\n",
    "logger.info(\"AI-Powered Data Cleaning Pipeline\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LOAD AI MODEL\n",
    "# ===================================================================\n",
    "\n",
    "def load_nlp_model():\n",
    "    \"\"\"Load the best available spaCy model.\"\"\"\n",
    "    if not SPACY_AVAILABLE:\n",
    "        logger.error(\"spaCy not installed!\")\n",
    "        return None\n",
    "    \n",
    "    # Try transformer model first (most accurate)\n",
    "    try:\n",
    "        logger.info(\"Loading spaCy Transformer model (en_core_web_trf)...\")\n",
    "        nlp = spacy.load(\"en_core_web_trf\")\n",
    "        logger.info(\"Loaded Transformer model - BEST ACCURACY\")\n",
    "        return nlp\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try large model\n",
    "    try:\n",
    "        logger.info(\"Loading spaCy Large model (en_core_web_lg)...\")\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        logger.info(\"Loaded Large model - GOOD ACCURACY\")\n",
    "        return nlp\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback to medium\n",
    "    try:\n",
    "        logger.info(\"Loading spaCy Medium model (en_core_web_md)...\")\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "        logger.info(\"Loaded Medium model - MODERATE ACCURACY\")\n",
    "        return nlp\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Last resort: small model\n",
    "    try:\n",
    "        logger.info(\"Loading spaCy Small model (en_core_web_sm)...\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        logger.info(\"Loaded Small model - BASIC ACCURACY\")\n",
    "        return nlp\n",
    "    except:\n",
    "        logger.error(\"No spaCy model found! Install with:\")\n",
    "        logger.error(\"python -m spacy download en_core_web_trf  # Best\")\n",
    "        logger.error(\"python -m spacy download en_core_web_lg   # Good\")\n",
    "        logger.error(\"python -m spacy download en_core_web_sm   # Basic\")\n",
    "        return None\n",
    "\n",
    "# Load model globally\n",
    "NLP_MODEL = load_nlp_model()\n",
    "\n",
    "# ===================================================================\n",
    "# 3. ENHANCED KNOWLEDGE BASE\n",
    "# ===================================================================\n",
    "\n",
    "KNOWLEDGE_BASE = {\n",
    "    # Technical Skills (Comprehensive)\n",
    "    \"skills\": {\n",
    "        # Programming Languages\n",
    "        \"python\", \"java\", \"javascript\", \"typescript\", \"c++\", \"c#\", \"c\", \"ruby\", \n",
    "        \"go\", \"golang\", \"rust\", \"swift\", \"kotlin\", \"scala\", \"r\", \"php\", \"perl\",\n",
    "        \n",
    "        # Web Frontend\n",
    "        \"react\", \"reactjs\", \"angular\", \"vue\", \"vuejs\", \"html\", \"css\", \"sass\",\n",
    "        \"bootstrap\", \"tailwind\", \"jquery\", \"webpack\", \"nextjs\", \"gatsby\",\n",
    "        \n",
    "        # Web Backend\n",
    "        \"node.js\", \"nodejs\", \"express\", \"django\", \"flask\", \"fastapi\", \"spring\",\n",
    "        \"spring boot\", \"asp.net\", \"laravel\", \"rails\", \"ruby on rails\",\n",
    "        \n",
    "        # Databases\n",
    "        \"sql\", \"mysql\", \"postgresql\", \"oracle\", \"mongodb\", \"redis\", \"cassandra\",\n",
    "        \"dynamodb\", \"elasticsearch\", \"neo4j\", \"sqlite\", \"mariadb\", \"couchdb\",\n",
    "        \n",
    "        # Cloud & DevOps\n",
    "        \"aws\", \"azure\", \"gcp\", \"google cloud\", \"docker\", \"kubernetes\", \"k8s\",\n",
    "        \"jenkins\", \"gitlab ci\", \"github actions\", \"terraform\", \"ansible\", \n",
    "        \"chef\", \"puppet\", \"circleci\", \"travis ci\",\n",
    "        \n",
    "        # Data Science & AI\n",
    "        \"machine learning\", \"ml\", \"deep learning\", \"ai\", \"artificial intelligence\",\n",
    "        \"tensorflow\", \"pytorch\", \"keras\", \"scikit-learn\", \"pandas\", \"numpy\",\n",
    "        \"data science\", \"nlp\", \"computer vision\", \"cv\", \"neural networks\",\n",
    "        \"transformers\", \"bert\", \"gpt\", \"llm\",\n",
    "        \n",
    "        # Mobile\n",
    "        \"android\", \"ios\", \"react native\", \"flutter\", \"xamarin\", \"swift\", \"kotlin\",\n",
    "        \n",
    "        # Tools & Platforms\n",
    "        \"git\", \"github\", \"gitlab\", \"bitbucket\", \"jira\", \"confluence\", \"slack\",\n",
    "        \"postman\", \"swagger\", \"figma\", \"sketch\", \"tableau\", \"power bi\",\n",
    "        \n",
    "        # Testing\n",
    "        \"selenium\", \"pytest\", \"junit\", \"jest\", \"cypress\", \"testing\", \"qa\",\n",
    "        \"test automation\", \"unit testing\", \"integration testing\",\n",
    "        \n",
    "        # Big Data\n",
    "        \"hadoop\", \"spark\", \"pyspark\", \"kafka\", \"airflow\", \"hive\", \"pig\",\n",
    "        \"flink\", \"storm\", \"hbase\",\n",
    "        \n",
    "        # Other\n",
    "        \"api\", \"rest\", \"restful\", \"graphql\", \"microservices\", \"agile\", \"scrum\",\n",
    "        \"blockchain\", \"web3\", \"solidity\", \"etl\", \"ci/cd\", \"linux\", \"unix\",\n",
    "        \"bash\", \"powershell\", \"sap\", \"salesforce\", \"excel\", \"vba\"\n",
    "    },\n",
    "    \n",
    "    # Job Positions\n",
    "    \"positions\": {\n",
    "        \"software engineer\", \"software developer\", \"data scientist\", \"data analyst\",\n",
    "        \"data engineer\", \"machine learning engineer\", \"ml engineer\", \"ai engineer\",\n",
    "        \"full stack developer\", \"fullstack developer\", \"frontend developer\",\n",
    "        \"front end developer\", \"backend developer\", \"back end developer\",\n",
    "        \"devops engineer\", \"cloud engineer\", \"cloud architect\", \"solutions architect\",\n",
    "        \"system administrator\", \"sysadmin\", \"network engineer\", \"database administrator\",\n",
    "        \"dba\", \"business analyst\", \"product manager\", \"project manager\",\n",
    "        \"qa engineer\", \"test engineer\", \"sdet\", \"automation engineer\",\n",
    "        \"security engineer\", \"cyber security analyst\", \"penetration tester\",\n",
    "        \"ui/ux designer\", \"ui designer\", \"ux designer\", \"graphic designer\",\n",
    "        \"technical lead\", \"tech lead\", \"team lead\", \"engineering manager\",\n",
    "        \"scrum master\", \"product owner\", \"consultant\", \"analyst\", \"developer\",\n",
    "        \"engineer\", \"architect\", \"specialist\", \"coordinator\", \"associate\",\n",
    "        \"intern\", \"trainee\", \"graduate trainee\", \"fresher\", \"junior\", \"senior\"\n",
    "    },\n",
    "    \n",
    "    # Locations (India + Global)\n",
    "    \"locations\": {\n",
    "        # Metro Cities\n",
    "        \"bangalore\", \"bengaluru\", \"mumbai\", \"delhi\", \"new delhi\", \"ncr\",\n",
    "        \"hyderabad\", \"pune\", \"chennai\", \"kolkata\", \"calcutta\",\n",
    "        \n",
    "        # Tier 2 Cities\n",
    "        \"ahmedabad\", \"gurgaon\", \"gurugram\", \"noida\", \"greater noida\",\n",
    "        \"chandigarh\", \"jaipur\", \"kochi\", \"cochin\", \"thiruvananthapuram\",\n",
    "        \"bhubaneswar\", \"indore\", \"coimbatore\", \"surat\", \"nagpur\", \"lucknow\",\n",
    "        \"vadodara\", \"visakhapatnam\", \"vizag\", \"mysore\", \"mysuru\",\n",
    "        \n",
    "        # Remote\n",
    "        \"remote\", \"work from home\", \"wfh\", \"hybrid\", \"anywhere\", \"pan india\"\n",
    "    },\n",
    "    \n",
    "    # Degrees\n",
    "    \"degrees\": {\n",
    "        \"b.tech\", \"btech\", \"be\", \"b.e\", \"bachelor of technology\",\n",
    "        \"bachelor of engineering\", \"m.tech\", \"mtech\", \"me\", \"m.e\",\n",
    "        \"master of technology\", \"master of engineering\", \"bca\", \"mca\",\n",
    "        \"bachelor of computer applications\", \"master of computer applications\",\n",
    "        \"bsc\", \"b.sc\", \"bachelor of science\", \"msc\", \"m.sc\", \"master of science\",\n",
    "        \"phd\", \"doctorate\", \"diploma\", \"mba\", \"pgdm\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Company patterns for regex\n",
    "COMPANY_PATTERNS = [\n",
    "    r'\\b([A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z]+){0,3})\\s+(?:Pvt\\.?\\s*Ltd\\.?|Private\\s+Limited|Limited|Ltd\\.?|Inc\\.?|Incorporated|Corp\\.?|Corporation|LLC|Technologies|Systems|Solutions|Software|Consulting|Services|Group|Enterprises|Industries)\\b',\n",
    "    r'\\b([A-Z][A-Za-z]{2,}(?:\\s+[A-Z][A-Za-z]+)?)\\s+(?:Tech|IT|Labs|Studio|Agency)\\b'\n",
    "]\n",
    "\n",
    "# Salary patterns\n",
    "SALARY_PATTERNS = [\n",
    "    r'(\\d+(?:\\.\\d+)?)\\s*(?:lpa|lakhs?\\s+per\\s+annum|l\\.?p\\.?a\\.?)',\n",
    "    r'(\\d+)\\s*(?:k|thousand)\\s*(?:per\\s+month|pm|/month)',\n",
    "    r'(?:ctc|package|salary)\\s*:?\\s*(?:rs\\.?\\s*)?(\\d+(?:\\.\\d+)?)\\s*(?:lpa|lakhs?)',\n",
    "    r'(?:stipend|compensation)\\s*:?\\s*(?:rs\\.?\\s*)?(\\d+(?:,\\d+)?)\\s*(?:per\\s+month)?'\n",
    "]\n",
    "\n",
    "# Experience patterns\n",
    "EXPERIENCE_PATTERNS = [\n",
    "    r'(\\d+)\\s*(?:\\+|to|-)\\s*(\\d+)\\s*years?\\s+(?:of\\s+)?experience',\n",
    "    r'(\\d+)\\s*years?\\s+(?:of\\s+)?experience',\n",
    "    r'\\b(fresher|freshers?)\\b',\n",
    "    r'\\b(entry\\s+level)\\b',\n",
    "    r'\\b0\\s*(?:\\+|to|-)\\s*(\\d+)\\s*years?\\b'\n",
    "]\n",
    "\n",
    "# ===================================================================\n",
    "# 4. AGGRESSIVE TEXT CLEANING\n",
    "# ===================================================================\n",
    "\n",
    "# Regex for cleaning\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMAIL_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "PHONE_RE = re.compile(r'[\\+]?[\\d][\\d\\s\\-\\(\\)]{7,}[\\d]')\n",
    "HTML_RE = re.compile(r'<[^>]+>')\n",
    "NON_PRINTABLE = re.compile(r'[^\\x20-\\x7E\\n\\r]+')\n",
    "\n",
    "REMOVE_PATTERNS = [\n",
    "    # Signatures\n",
    "    r'(?:best\\s+)?regards?,?.*',\n",
    "    r'thanks?\\s+(?:and\\s+)?regards?.*',\n",
    "    r'sincerely,?.*',\n",
    "    r'warm\\s+regards?.*',\n",
    "    \n",
    "    # Email artifacts\n",
    "    r'------+\\s*forwarded\\s+message\\s*------+.*',\n",
    "    r'from:.*?(?:sent:|to:|subject:).*',\n",
    "    r'----+\\s*original\\s+message\\s*----+.*',\n",
    "    r'on\\s+\\w+,\\s+\\w+\\s+\\d+,\\s+\\d+.*?wrote:.*',\n",
    "    r'(?:fwd?|fw|re):.*?\\n',\n",
    "    \n",
    "    # Disclaimers\n",
    "    r'this\\s+(?:e-?mail|message).*?confidential.*',\n",
    "    r'disclaimer:.*',\n",
    "    r'confidentiality\\s+notice:.*',\n",
    "    r'(?:please\\s+)?do\\s+not\\s+reply.*',\n",
    "    r'this\\s+is\\s+an?\\s+automated.*',\n",
    "    r'unsubscribe.*',\n",
    "    r'sent\\s+from\\s+my.*',\n",
    "    r'get\\s+outlook\\s+for.*'\n",
    "]\n",
    "\n",
    "def clean_text_ai(text: str) -> str:\n",
    "    \"\"\"AI-ready text cleaning - removes junk, keeps semantic content.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # Remove HTML\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    \n",
    "    # Remove URLs, emails, phones\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = EMAIL_RE.sub(\" \", text)\n",
    "    text = PHONE_RE.sub(\" \", text)\n",
    "    \n",
    "    # Remove all unwanted patterns\n",
    "    for pattern in REMOVE_PATTERNS:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Remove non-printable\n",
    "    text = NON_PRINTABLE.sub(\" \", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\n\\s*\\n+', \"\\n\", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    # Remove very short lines (likely artifacts)\n",
    "    lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 5]\n",
    "    text = \" \".join(lines)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# ===================================================================\n",
    "# 5. AI-POWERED ENTITY EXTRACTION\n",
    "# ===================================================================\n",
    "\n",
    "def extract_with_ai(text: str, nlp_model) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use spaCy NER + Pattern Matching for superior extraction.\n",
    "    This is MUCH better than pure regex.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'companies': set(),\n",
    "        'skills': set(),\n",
    "        'positions': set(),\n",
    "        'locations': set(),\n",
    "        'salary_info': [],\n",
    "        'experience_required': [],\n",
    "        'degrees_required': set()\n",
    "    }\n",
    "    \n",
    "    if not nlp_model or not text:\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        # Process with spaCy (uses AI for context understanding)\n",
    "        doc = nlp_model(text[:5000])  # Limit for performance\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # 1. Extract ORGANIZATIONS using NER (AI-powered)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                # Clean and validate\n",
    "                org = ent.text.strip()\n",
    "                if len(org) > 2 and not org.lower() in ['we', 'our', 'the', 'a', 'an']:\n",
    "                    result['companies'].add(org)\n",
    "        \n",
    "        # 2. Also use regex for company patterns\n",
    "        for pattern in COMPANY_PATTERNS:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                company = match.group(0).strip()\n",
    "                if len(company) > 3:\n",
    "                    result['companies'].add(company)\n",
    "        \n",
    "        # 3. Extract SKILLS using POS tagging + matching\n",
    "        # This is smarter than pure keyword matching\n",
    "        for token in doc:\n",
    "            token_lower = token.text.lower()\n",
    "            # Check if token or its lemma matches skills\n",
    "            if token_lower in KNOWLEDGE_BASE['skills']:\n",
    "                result['skills'].add(token_lower)\n",
    "            elif token.lemma_.lower() in KNOWLEDGE_BASE['skills']:\n",
    "                result['skills'].add(token.lemma_.lower())\n",
    "        \n",
    "        # Multi-word skills (using noun chunks for context)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_lower = chunk.text.lower()\n",
    "            if chunk_lower in KNOWLEDGE_BASE['skills']:\n",
    "                result['skills'].add(chunk_lower)\n",
    "        \n",
    "        # 4. Extract POSITIONS using noun phrases + matching\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_lower = chunk.text.lower()\n",
    "            if chunk_lower in KNOWLEDGE_BASE['positions']:\n",
    "                result['positions'].add(chunk_lower)\n",
    "        \n",
    "        # Also check individual tokens\n",
    "        for token in doc:\n",
    "            token_lower = token.text.lower()\n",
    "            if token_lower in KNOWLEDGE_BASE['positions']:\n",
    "                result['positions'].add(token_lower)\n",
    "        \n",
    "        # 5. Extract LOCATIONS using NER (GPE = Geo-Political Entity)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\":\n",
    "                location = ent.text.lower().strip()\n",
    "                if location in KNOWLEDGE_BASE['locations']:\n",
    "                    result['locations'].add(location)\n",
    "        \n",
    "        # Also direct matching for locations\n",
    "        for location in KNOWLEDGE_BASE['locations']:\n",
    "            if location in text_lower:\n",
    "                result['locations'].add(location)\n",
    "        \n",
    "        # 6. Extract SALARY using patterns\n",
    "        for pattern in SALARY_PATTERNS:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                result['salary_info'].append(match.group(0).strip())\n",
    "        \n",
    "        # 7. Extract EXPERIENCE using patterns\n",
    "        for pattern in EXPERIENCE_PATTERNS:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                result['experience_required'].append(match.group(0).strip())\n",
    "        \n",
    "        # 8. Extract DEGREES\n",
    "        for degree in KNOWLEDGE_BASE['degrees']:\n",
    "            pattern = r'\\b' + re.escape(degree) + r'\\b'\n",
    "            if re.search(pattern, text_lower):\n",
    "                result['degrees_required'].add(degree)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"AI extraction error: {str(e)[:100]}\")\n",
    "    \n",
    "    # Convert sets to sorted lists\n",
    "    result['companies'] = sorted(list(result['companies']))\n",
    "    result['skills'] = sorted(list(result['skills']))\n",
    "    result['positions'] = sorted(list(result['positions']))\n",
    "    result['locations'] = sorted(list(result['locations']))\n",
    "    result['degrees_required'] = sorted(list(result['degrees_required']))\n",
    "    \n",
    "    # Remove duplicates from lists\n",
    "    result['salary_info'] = sorted(list(set(result['salary_info'])))\n",
    "    result['experience_required'] = sorted(list(set(result['experience_required'])))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ===================================================================\n",
    "# 6. MAIN PROCESSING FUNCTION\n",
    "# ===================================================================\n",
    "\n",
    "def process_email_ai(text: str, nlp_model) -> Dict[str, Any]:\n",
    "    \"\"\"Process single email with AI model.\"\"\"\n",
    "    result = {\n",
    "        'cleaned_text': '',\n",
    "        'companies': [],\n",
    "        'skills': [],\n",
    "        'positions': [],\n",
    "        'locations': [],\n",
    "        'salary_info': [],\n",
    "        'experience_required': [],\n",
    "        'degrees_required': [],\n",
    "        'word_count': 0,\n",
    "        'char_count': 0,\n",
    "        'processing_status': 'success'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Clean text\n",
    "        cleaned = clean_text_ai(text)\n",
    "        \n",
    "        if not cleaned or len(cleaned) < 10:\n",
    "            result['processing_status'] = 'empty_after_cleaning'\n",
    "            return result\n",
    "        \n",
    "        result['cleaned_text'] = cleaned\n",
    "        result['word_count'] = len(cleaned.split())\n",
    "        result['char_count'] = len(cleaned)\n",
    "        \n",
    "        # Step 2: AI-powered extraction\n",
    "        if nlp_model:\n",
    "            extracted = extract_with_ai(cleaned, nlp_model)\n",
    "            result.update(extracted)\n",
    "        else:\n",
    "            result['processing_status'] = 'no_ai_model'\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['processing_status'] = f'error: {str(e)[:100]}'\n",
    "        logger.error(f\"Processing error: {str(e)}\")\n",
    "        return result\n",
    "\n",
    "# ===================================================================\n",
    "# 7. BATCH PROCESSING\n",
    "# ===================================================================\n",
    "\n",
    "def process_batch_ai(texts: List[str], nlp_model, batch_size: int = 20) -> List[Dict]:\n",
    "    \"\"\"Process emails in batches with AI model.\"\"\"\n",
    "    total = len(texts)\n",
    "    total_batches = (total + batch_size - 1) // batch_size\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\"AI-POWERED DATA CLEANING\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    logger.info(f\"Total Emails: {total}\")\n",
    "    logger.info(f\"Batches: {total_batches} (size: {batch_size})\")\n",
    "    logger.info(f\"Model: {nlp_model.meta['name'] if nlp_model else 'None'}\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'with_companies': 0,\n",
    "        'with_skills': 0,\n",
    "        'with_positions': 0,\n",
    "        'with_locations': 0,\n",
    "        'with_salary': 0,\n",
    "        'empty': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_num in range(0, total, batch_size):\n",
    "        batch_texts = texts[batch_num:batch_num + batch_size]\n",
    "        current_batch = batch_num // batch_size + 1\n",
    "        \n",
    "        logger.info(f\"Batch {current_batch}/{total_batches} ({len(batch_texts)} emails)...\")\n",
    "        \n",
    "        batch_start = time.time()\n",
    "        batch_results = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            result = process_email_ai(text, nlp_model)\n",
    "            batch_results.append(result)\n",
    "            \n",
    "            # Update stats\n",
    "            stats['total'] += 1\n",
    "            if result['companies']: stats['with_companies'] += 1\n",
    "            if result['skills']: stats['with_skills'] += 1\n",
    "            if result['positions']: stats['with_positions'] += 1\n",
    "            if result['locations']: stats['with_locations'] += 1\n",
    "            if result['salary_info']: stats['with_salary'] += 1\n",
    "            if result['processing_status'] == 'empty_after_cleaning':\n",
    "                stats['empty'] += 1\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        logger.info(f\"Batch {current_batch}/{total_batches} | Time: {batch_time:.2f}s | \"\n",
    "                   f\"Companies: {stats['with_companies']} | Skills: {stats['with_skills']}\")\n",
    "        \n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\"AI PROCESSING COMPLETE\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    logger.info(f\"Total Time: {total_time:.2f}s ({total/total_time:.1f} emails/sec)\")\n",
    "    logger.info(f\"Total Processed: {stats['total']}\")\n",
    "    logger.info(f\"With Companies: {stats['with_companies']} ({stats['with_companies']/stats['total']*100:.1f}%)\")\n",
    "    logger.info(f\"With Skills: {stats['with_skills']} ({stats['with_skills']/stats['total']*100:.1f}%)\")\n",
    "    logger.info(f\"With Positions: {stats['with_positions']} ({stats['with_positions']/stats['total']*100:.1f}%)\")\n",
    "    logger.info(f\"With Locations: {stats['with_locations']} ({stats['with_locations']/stats['total']*100:.1f}%)\")\n",
    "    logger.info(f\"With Salary: {stats['with_salary']} ({stats['with_salary']/stats['total']*100:.1f}%)\")\n",
    "    logger.info(f\"Empty After Clean: {stats['empty']}\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# ===================================================================\n",
    "# 8. MAIN PIPELINE\n",
    "# ===================================================================\n",
    "\n",
    "def main_ai_pipeline(csv_path: str, output_path: str = \"ai_cleaned_emails.csv\"):\n",
    "    \"\"\"Main AI-powered cleaning pipeline.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading dataset: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine text columns\n",
    "    df['combined_text'] = (\n",
    "        df['Subject'].astype(str).fillna('') + ' ' + \n",
    "        df['Preview'].astype(str).fillna('') + ' ' + \n",
    "        df['Body'].astype(str).fillna('')\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Loaded {len(df)} emails\\n\")\n",
    "    \n",
    "    # Check if model is available\n",
    "    if not NLP_MODEL:\n",
    "        logger.error(\"Cannot proceed without NLP model!\")\n",
    "        logger.error(\"Install spaCy model with:\")\n",
    "        logger.error(\"  python -m spacy download en_core_web_trf  # Transformer (Best)\")\n",
    "        logger.error(\"  python -m spacy download en_core_web_lg   # Large (Good)\")\n",
    "        logger.error(\"  python -m spacy download en_core_web_sm   # Small (Basic)\")\n",
    "        return None\n",
    "    \n",
    "    # Process all emails\n",
    "    texts = df['combined_text'].tolist()\n",
    "    results = process_batch_ai(texts, NLP_MODEL, batch_size=20)\n",
    "    \n",
    "    # Create enriched DataFrame\n",
    "    logger.info(\"Creating enriched dataset...\")\n",
    "    \n",
    "    df['cleaned_text'] = [r['cleaned_text'] for r in results]\n",
    "    df['word_count'] = [r['word_count'] for r in results]\n",
    "    df['char_count'] = [r['char_count'] for r in results]\n",
    "    df['processing_status'] = [r['processing_status'] for r in results]\n",
    "    \n",
    "    # Entity columns\n",
    "    df['companies_extracted'] = [', '.join(r['companies']) for r in results]\n",
    "    df['skills_extracted'] = [', '.join(r['skills']) for r in results]\n",
    "    df['positions_extracted'] = [', '.join(r['positions']) for r in results]\n",
    "    df['locations_extracted'] = [', '.join(r['locations']) for r in results]\n",
    "    df['salary_info'] = [', '.join(r['salary_info']) for r in results]\n",
    "    df['experience_required'] = [', '.join(r['experience_required']) for r in results]\n",
    "    df['degrees_required'] = [', '.join(r['degrees_required']) for r in results]\n",
    "    \n",
    "    # Count columns\n",
    "    df['company_count'] = [len(r['companies']) for r in results]\n",
    "    df['skill_count'] = [len(r['skills']) for r in results]\n",
    "    df['position_count'] = [len(r['positions']) for r in results]\n",
    "    df['location_count'] = [len(r['locations']) for r in results]\n",
    "    \n",
    "    # Save\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    # Generate insights\n",
    "    generate_insights(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ===================================================================\n",
    "# 9. INSIGHTS GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "def generate_insights(df: pd.DataFrame):\n",
    "    \"\"\"Generate detailed insights from extracted data.\"\"\"\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\"EXTRACTION INSIGHTS\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Companies\n",
    "    all_companies = []\n",
    "    for comp_str in df['companies_extracted']:\n",
    "        if comp_str:\n",
    "            all_companies.extend([c.strip() for c in comp_str.split(',')])\n",
    "    \n",
    "    if all_companies:\n",
    "        company_counts = Counter(all_companies)\n",
    "        logger.info(f\"TOP 20 COMPANIES:\")\n",
    "        for company, count in company_counts.most_common(20):\n",
    "            logger.info(f\"   {company}: {count} times\")\n",
    "    \n",
    "    # Skills\n",
    "    all_skills = []\n",
    "    for skill_str in df['skills_extracted']:\n",
    "        if skill_str:\n",
    "            all_skills.extend([s.strip() for s in skill_str.split(',')])\n",
    "    \n",
    "    if all_skills:\n",
    "        skill_counts = Counter(all_skills)\n",
    "        logger.info(f\"\\nTOP 25 SKILLS:\")\n",
    "        for skill, count in skill_counts.most_common(25):\n",
    "            logger.info(f\"   {skill}: {count} times\")\n",
    "    \n",
    "    # Positions\n",
    "    all_positions = []\n",
    "    for pos_str in df['positions_extracted']:\n",
    "        if pos_str:\n",
    "            all_positions.extend([p.strip() for p in pos_str.split(',')])\n",
    "    \n",
    "    if all_positions:\n",
    "        position_counts = Counter(all_positions)\n",
    "        logger.info(f\"\\nTOP 15 POSITIONS:\")\n",
    "        for position, count in position_counts.most_common(15):\n",
    "            logger.info(f\"   {position}: {count} times\")\n",
    "    \n",
    "    # Locations\n",
    "    all_locations = []\n",
    "    for loc_str in df['locations_extracted']:\n",
    "        if loc_str:\n",
    "            all_locations.extend([l.strip() for l in loc_str.split(',')])\n",
    "    \n",
    "    if all_locations:\n",
    "        location_counts = Counter(all_locations)\n",
    "        logger.info(f\"\\nTOP 10 LOCATIONS:\")\n",
    "        for location, count in location_counts.most_common(10):\n",
    "            logger.info(f\"   {location}: {count} times\")\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# 10. USAGE\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH = r\"D:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Phase_scripts\\Phase 1\\placement_emails.csv\"\n",
    "    OUTPUT_PATH = \"ai_cleaned_emails.csv\"\n",
    "    \n",
    "    df = main_ai_pipeline(CSV_PATH, OUTPUT_PATH)\n",
    "    \n",
    "    if df is not None:\n",
    "        logger.info(\"AI-powered pipeline complete!\")\n",
    "        logger.info(f\"Check output: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
