{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PHASE 5B: RAG SYSTEM WITH VECTOR DATABASE\n",
    "# Retrieval Augmented Generation for intelligent job search\n",
    "# ===================================================================\n",
    "# Purpose: Semantic search and intelligent data retrieval for jobs\n",
    "# Dependencies: chromadb, sentence-transformers, pandas\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ===================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ===================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"rag_system.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"RAGSystem\")\n",
    "\n",
    "# ===================================================================\n",
    "# VECTOR DATABASE MANAGER\n",
    "# ===================================================================\n",
    "\n",
    "class VectorDatabaseManager:\n",
    "    \"\"\"\n",
    "    Manages ChromaDB vector database for semantic search.\n",
    "    Stores job embeddings for fast similarity search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        persist_directory: str = \"./vector_db\",\n",
    "        collection_name: str = \"job_postings\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize vector database.\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Directory to persist the database\n",
    "            collection_name: Name of the collection\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(\"VectorDB\")\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(\"üóÑÔ∏è  INITIALIZING VECTOR DATABASE\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "        \n",
    "        # Import ChromaDB\n",
    "        try:\n",
    "            import chromadb\n",
    "            from chromadb.config import Settings\n",
    "            self.chromadb = chromadb\n",
    "        except ImportError:\n",
    "            self.logger.error(\"‚ùå chromadb not installed!\")\n",
    "            self.logger.error(\"Install with: pip install chromadb\")\n",
    "            raise\n",
    "        \n",
    "        # Create persist directory if not exists\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        \n",
    "        # Initialize ChromaDB client\n",
    "        try:\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=persist_directory,\n",
    "                settings=Settings(\n",
    "                    anonymized_telemetry=False,\n",
    "                    allow_reset=True\n",
    "                )\n",
    "            )\n",
    "            self.logger.info(f\"‚úÖ ChromaDB client initialized at {persist_directory}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Failed to initialize ChromaDB: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Get or create collection\n",
    "        try:\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"description\": \"Job postings for semantic search\"}\n",
    "            )\n",
    "            self.logger.info(f\"‚úÖ Collection '{collection_name}' ready\")\n",
    "            self.logger.info(f\"üìä Current documents: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Failed to create collection: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.logger.info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[str],\n",
    "        metadatas: List[Dict[str, Any]],\n",
    "        ids: List[str]\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Add documents to vector database.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to embed\n",
    "            metadatas: List of metadata dictionaries\n",
    "            ids: List of unique document IDs\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"üìù Adding {len(documents)} documents to vector DB...\")\n",
    "            \n",
    "            # Add to collection (ChromaDB handles embedding automatically)\n",
    "            self.collection.add(\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Successfully added {len(documents)} documents\")\n",
    "            self.logger.info(f\"üìä Total documents: {self.collection.count()}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error adding documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        n_results: int = 10,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Semantic search for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            n_results: Number of results to return\n",
    "            filters: Optional metadata filters\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"üîç Searching for: '{query}' (top {n_results})\")\n",
    "            \n",
    "            # Perform semantic search\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results,\n",
    "                where=filters  # Optional metadata filtering\n",
    "            )\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = {\n",
    "                'query': query,\n",
    "                'n_results': len(results['ids'][0]),\n",
    "                'documents': results['documents'][0],\n",
    "                'metadatas': results['metadatas'][0],\n",
    "                'distances': results['distances'][0],\n",
    "                'ids': results['ids'][0]\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Found {len(results['ids'][0])} results\")\n",
    "            \n",
    "            return formatted_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Search error: {e}\")\n",
    "            return {'query': query, 'n_results': 0, 'documents': [], 'metadatas': [], 'ids': []}\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        \"\"\"Delete the entire collection.\"\"\"\n",
    "        try:\n",
    "            self.client.delete_collection(name=self.collection_name)\n",
    "            self.logger.info(f\"üóëÔ∏è  Deleted collection '{self.collection_name}'\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error deleting collection: {e}\")\n",
    "    \n",
    "    def reset_collection(self):\n",
    "        \"\"\"Reset collection (delete and recreate).\"\"\"\n",
    "        try:\n",
    "            self.delete_collection()\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Job postings for semantic search\"}\n",
    "            )\n",
    "            self.logger.info(f\"üîÑ Collection '{self.collection_name}' reset\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error resetting collection: {e}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# JOB DATA INDEXER\n",
    "# ===================================================================\n",
    "\n",
    "class JobDataIndexer:\n",
    "    \"\"\"\n",
    "    Indexes job data into vector database.\n",
    "    Creates searchable embeddings from job postings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db: VectorDatabaseManager):\n",
    "        \"\"\"\n",
    "        Initialize job indexer.\n",
    "        \n",
    "        Args:\n",
    "            vector_db: VectorDatabaseManager instance\n",
    "        \"\"\"\n",
    "        self.vector_db = vector_db\n",
    "        self.logger = logging.getLogger(\"JobIndexer\")\n",
    "    \n",
    "    def create_job_document(self, job: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Create searchable document text from job data.\n",
    "        Combines all relevant fields into a rich text representation.\n",
    "        \n",
    "        Args:\n",
    "            job: Job data as pandas Series\n",
    "            \n",
    "        Returns:\n",
    "            Formatted document string\n",
    "        \"\"\"\n",
    "        # Build comprehensive document\n",
    "        doc_parts = []\n",
    "        \n",
    "        # Position and company\n",
    "        doc_parts.append(f\"Position: {job.get('position_title', 'Unknown')}\")\n",
    "        doc_parts.append(f\"Company: {job.get('company_name', 'Unknown')}\")\n",
    "        \n",
    "        # Location and work mode\n",
    "        location = job.get('location_city', 'Unknown')\n",
    "        work_mode = job.get('work_mode', 'On-site')\n",
    "        doc_parts.append(f\"Location: {location} ({work_mode})\")\n",
    "        \n",
    "        # Skills\n",
    "        skills = job.get('skills_required', '')\n",
    "        if pd.notna(skills) and skills:\n",
    "            doc_parts.append(f\"Required Skills: {skills}\")\n",
    "        \n",
    "        # Experience\n",
    "        exp_type = job.get('experience_type', 'Not specified')\n",
    "        exp_min = job.get('experience_min_years', 0)\n",
    "        exp_max = job.get('experience_max_years', 0)\n",
    "        if exp_max > 0:\n",
    "            doc_parts.append(f\"Experience: {exp_min}-{exp_max} years ({exp_type})\")\n",
    "        else:\n",
    "            doc_parts.append(f\"Experience: {exp_type}\")\n",
    "        \n",
    "        # Education\n",
    "        education = job.get('education_required', '')\n",
    "        if pd.notna(education) and education:\n",
    "            doc_parts.append(f\"Education: {education}\")\n",
    "        \n",
    "        # Salary\n",
    "        salary_max = job.get('salary_max', 0)\n",
    "        if salary_max > 0:\n",
    "            salary_lpa = salary_max / 100000\n",
    "            doc_parts.append(f\"Salary: ‚Çπ{salary_lpa:.1f} LPA\")\n",
    "        \n",
    "        # Priority score\n",
    "        priority = job.get('final_priority_score', 0)\n",
    "        doc_parts.append(f\"Priority Score: {priority:.1f}/100\")\n",
    "        \n",
    "        # Combine all parts\n",
    "        document = \". \".join(doc_parts)\n",
    "        \n",
    "        return document\n",
    "    \n",
    "    def create_job_metadata(self, job: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create metadata dictionary for filtering and retrieval.\n",
    "        \n",
    "        Args:\n",
    "            job: Job data as pandas Series\n",
    "            \n",
    "        Returns:\n",
    "            Metadata dictionary\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            'job_id': str(job.get('job_id', '')),\n",
    "            'company_name': str(job.get('company_name', '')),\n",
    "            'position_title': str(job.get('position_title', '')),\n",
    "            'location_city': str(job.get('location_city', '')),\n",
    "            'work_mode': str(job.get('work_mode', 'On-site')),\n",
    "            'salary_max': float(job.get('salary_max', 0)),\n",
    "            'priority_score': float(job.get('final_priority_score', 0)),\n",
    "            'experience_type': str(job.get('experience_type', '')),\n",
    "            'skill_count': int(job.get('skills_count', 0)) if pd.notna(job.get('skills_count')) else 0\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def index_jobs(self, jobs_df: pd.DataFrame, batch_size: int = 100) -> bool:\n",
    "        \"\"\"\n",
    "        Index all jobs into vector database.\n",
    "        \n",
    "        Args:\n",
    "            jobs_df: DataFrame with job postings\n",
    "            batch_size: Number of jobs to index per batch\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"üìö INDEXING {len(jobs_df)} JOBS INTO VECTOR DATABASE\")\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        total_jobs = len(jobs_df)\n",
    "        total_batches = (total_jobs + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_num in range(0, total_jobs, batch_size):\n",
    "            batch_jobs = jobs_df.iloc[batch_num:batch_num + batch_size]\n",
    "            current_batch = batch_num // batch_size + 1\n",
    "            \n",
    "            self.logger.info(f\"üì¶ Processing batch {current_batch}/{total_batches} ({len(batch_jobs)} jobs)...\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch data\n",
    "                documents = []\n",
    "                metadatas = []\n",
    "                ids = []\n",
    "                \n",
    "                for idx, job in batch_jobs.iterrows():\n",
    "                    # Create document\n",
    "                    doc = self.create_job_document(job)\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "                    # Create metadata\n",
    "                    metadata = self.create_job_metadata(job)\n",
    "                    metadatas.append(metadata)\n",
    "                    \n",
    "                    # Create unique ID\n",
    "                    job_id = job.get('job_id', f\"job_{idx}\")\n",
    "                    ids.append(str(job_id))\n",
    "                \n",
    "                # Add batch to vector DB\n",
    "                success = self.vector_db.add_documents(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids\n",
    "                )\n",
    "                \n",
    "                if success:\n",
    "                    self.logger.info(f\"‚úÖ Batch {current_batch}/{total_batches} indexed successfully\\n\")\n",
    "                else:\n",
    "                    self.logger.error(f\"‚ùå Failed to index batch {current_batch}\")\n",
    "                    return False\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"‚ùå Error indexing batch {current_batch}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        self.logger.info(f\"{'='*70}\")\n",
    "        self.logger.info(f\"‚úÖ INDEXING COMPLETE - {total_jobs} JOBS INDEXED\")\n",
    "        self.logger.info(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# INTELLIGENT RETRIEVAL SYSTEM\n",
    "# ===================================================================\n",
    "\n",
    "class IntelligentRetriever:\n",
    "    \"\"\"\n",
    "    Intelligent retrieval system using RAG.\n",
    "    Combines semantic search with filtering and ranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_db: VectorDatabaseManager,\n",
    "        jobs_df: pd.DataFrame\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize retriever.\n",
    "        \n",
    "        Args:\n",
    "            vector_db: VectorDatabaseManager instance\n",
    "            jobs_df: DataFrame with full job data\n",
    "        \"\"\"\n",
    "        self.vector_db = vector_db\n",
    "        self.jobs_df = jobs_df\n",
    "        self.logger = logging.getLogger(\"IntelligentRetriever\")\n",
    "        \n",
    "        # Create job_id to index mapping\n",
    "        self.job_index_map = {\n",
    "            str(job_id): idx \n",
    "            for idx, job_id in enumerate(jobs_df['job_id'])\n",
    "        }\n",
    "    \n",
    "    def retrieve_jobs(\n",
    "        self,\n",
    "        query: str,\n",
    "        n_results: int = 10,\n",
    "        filters: Optional[Dict[str, Any]] = None,\n",
    "        rerank: bool = True\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant jobs using semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            n_results: Number of results to return\n",
    "            filters: Optional metadata filters\n",
    "            rerank: Whether to rerank results by priority score\n",
    "            \n",
    "        Returns:\n",
    "            List of job dictionaries with relevance scores\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\n{'‚îÄ'*70}\")\n",
    "        self.logger.info(f\"üîç RETRIEVING JOBS\")\n",
    "        self.logger.info(f\"Query: {query}\")\n",
    "        self.logger.info(f\"Filters: {filters}\")\n",
    "        self.logger.info(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Semantic search\n",
    "            search_results = self.vector_db.search(\n",
    "                query=query,\n",
    "                n_results=n_results * 2,  # Get more for reranking\n",
    "                filters=filters\n",
    "            )\n",
    "            \n",
    "            if search_results['n_results'] == 0:\n",
    "                self.logger.warning(\"‚ö†Ô∏è  No results found\")\n",
    "                return []\n",
    "            \n",
    "            # Step 2: Enrich with full job data\n",
    "            enriched_results = []\n",
    "            \n",
    "            for i, job_id in enumerate(search_results['ids']):\n",
    "                try:\n",
    "                    # Get full job data\n",
    "                    if job_id in self.job_index_map:\n",
    "                        idx = self.job_index_map[job_id]\n",
    "                        job_data = self.jobs_df.iloc[idx].to_dict()\n",
    "                        \n",
    "                        # Add search metadata\n",
    "                        job_data['semantic_distance'] = search_results['distances'][i]\n",
    "                        job_data['search_rank'] = i + 1\n",
    "                        \n",
    "                        # Calculate relevance score (lower distance = higher relevance)\n",
    "                        # Distance is typically 0-2, we invert it to 0-100 scale\n",
    "                        relevance = max(0, 100 - (search_results['distances'][i] * 50))\n",
    "                        job_data['relevance_score'] = relevance\n",
    "                        \n",
    "                        enriched_results.append(job_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error enriching job {job_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Step 3: Rerank by priority score if enabled\n",
    "            if rerank and enriched_results:\n",
    "                self.logger.info(\"üìä Reranking by combined score...\")\n",
    "                \n",
    "                for job in enriched_results:\n",
    "                    # Combined score: 60% relevance + 40% priority\n",
    "                    job['combined_score'] = (\n",
    "                        job['relevance_score'] * 0.6 + \n",
    "                        job['final_priority_score'] * 0.4\n",
    "                    )\n",
    "                \n",
    "                enriched_results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "            \n",
    "            # Step 4: Limit to requested number\n",
    "            final_results = enriched_results[:n_results]\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Retrieved {len(final_results)} relevant jobs\")\n",
    "            self.logger.info(f\"{'‚îÄ'*70}\\n\")\n",
    "            \n",
    "            return final_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Retrieval error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def retrieve_by_filters(\n",
    "        self,\n",
    "        skills: Optional[List[str]] = None,\n",
    "        locations: Optional[List[str]] = None,\n",
    "        companies: Optional[List[str]] = None,\n",
    "        min_salary: Optional[float] = None,\n",
    "        work_mode: Optional[str] = None,\n",
    "        n_results: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve jobs using structured filters.\n",
    "        \n",
    "        Args:\n",
    "            skills: List of required skills\n",
    "            locations: List of preferred locations\n",
    "            companies: List of preferred companies\n",
    "            min_salary: Minimum salary in INR\n",
    "            work_mode: Work mode preference\n",
    "            n_results: Number of results\n",
    "            \n",
    "        Returns:\n",
    "            List of filtered job dictionaries\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\\n{'‚îÄ'*70}\")\n",
    "        self.logger.info(f\"üîß FILTERING JOBS\")\n",
    "        self.logger.info(f\"Skills: {skills}\")\n",
    "        self.logger.info(f\"Locations: {locations}\")\n",
    "        self.logger.info(f\"Companies: {companies}\")\n",
    "        self.logger.info(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Start with all jobs\n",
    "            filtered_df = self.jobs_df.copy()\n",
    "            \n",
    "            # Apply filters\n",
    "            if skills:\n",
    "                skill_pattern = '|'.join(skills)\n",
    "                filtered_df = filtered_df[\n",
    "                    filtered_df['skills_required'].str.contains(\n",
    "                        skill_pattern, case=False, na=False\n",
    "                    )\n",
    "                ]\n",
    "            \n",
    "            if locations:\n",
    "                location_pattern = '|'.join(locations)\n",
    "                filtered_df = filtered_df[\n",
    "                    filtered_df['location_city'].str.contains(\n",
    "                        location_pattern, case=False, na=False\n",
    "                    )\n",
    "                ]\n",
    "            \n",
    "            if companies:\n",
    "                company_pattern = '|'.join(companies)\n",
    "                filtered_df = filtered_df[\n",
    "                    filtered_df['company_name'].str.contains(\n",
    "                        company_pattern, case=False, na=False\n",
    "                    )\n",
    "                ]\n",
    "            \n",
    "            if min_salary:\n",
    "                filtered_df = filtered_df[filtered_df['salary_max'] >= min_salary]\n",
    "            \n",
    "            if work_mode:\n",
    "                filtered_df = filtered_df[\n",
    "                    filtered_df['work_mode'].str.contains(\n",
    "                        work_mode, case=False, na=False\n",
    "                    )\n",
    "                ]\n",
    "            \n",
    "            # Sort by priority score\n",
    "            filtered_df = filtered_df.sort_values('final_priority_score', ascending=False)\n",
    "            \n",
    "            # Limit results\n",
    "            filtered_df = filtered_df.head(n_results)\n",
    "            \n",
    "            # Convert to list of dictionaries\n",
    "            results = filtered_df.to_dict('records')\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Filtered to {len(results)} jobs\")\n",
    "            self.logger.info(f\"{'‚îÄ'*70}\\n\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Filtering error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_job_by_id(self, job_id: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get specific job by ID.\n",
    "        \n",
    "        Args:\n",
    "            job_id: Job identifier\n",
    "            \n",
    "        Returns:\n",
    "            Job dictionary or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if job_id in self.job_index_map:\n",
    "                idx = self.job_index_map[job_id]\n",
    "                return self.jobs_df.iloc[idx].to_dict()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting job {job_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# RAG SYSTEM ORCHESTRATOR\n",
    "# ===================================================================\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Complete RAG system orchestrator.\n",
    "    Combines vector database, indexing, and retrieval.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        jobs_csv_path: str = \"prioritized_jobs.csv\",\n",
    "        vector_db_path: str = \"./vector_db\",\n",
    "        rebuild_index: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize complete RAG system.\n",
    "        \n",
    "        Args:\n",
    "            jobs_csv_path: Path to jobs CSV\n",
    "            vector_db_path: Path to vector database\n",
    "            rebuild_index: Whether to rebuild the index\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(\"RAGSystem\")\n",
    "        \n",
    "        self.logger.info(\"\\n\" + \"=\"*70)\n",
    "        self.logger.info(\"üöÄ INITIALIZING RAG SYSTEM\")\n",
    "        self.logger.info(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Load jobs data\n",
    "        try:\n",
    "            self.jobs_df = pd.read_csv(jobs_csv_path)\n",
    "            self.logger.info(f\"‚úÖ Loaded {len(self.jobs_df)} jobs from {jobs_csv_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Failed to load jobs: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Initialize vector database\n",
    "        self.vector_db = VectorDatabaseManager(\n",
    "            persist_directory=vector_db_path\n",
    "        )\n",
    "        \n",
    "        # Initialize indexer and retriever\n",
    "        self.indexer = JobDataIndexer(self.vector_db)\n",
    "        self.retriever = IntelligentRetriever(self.vector_db, self.jobs_df)\n",
    "        \n",
    "        # Build or rebuild index\n",
    "        if rebuild_index or self.vector_db.collection.count() == 0:\n",
    "            self.logger.info(\"üî® Building vector index...\")\n",
    "            self.build_index()\n",
    "        else:\n",
    "            self.logger.info(f\"‚úÖ Using existing index ({self.vector_db.collection.count()} documents)\")\n",
    "        \n",
    "        self.logger.info(\"\\n\" + \"=\"*70)\n",
    "        self.logger.info(\"‚úÖ RAG SYSTEM READY\")\n",
    "        self.logger.info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def build_index(self) -> bool:\n",
    "        \"\"\"Build vector index from jobs data.\"\"\"\n",
    "        return self.indexer.index_jobs(self.jobs_df)\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        n_results: int = 10,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for jobs using natural language.\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            n_results: Number of results\n",
    "            filters: Optional filters\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant jobs\n",
    "        \"\"\"\n",
    "        return self.retriever.retrieve_jobs(query, n_results, filters)\n",
    "    \n",
    "    def filter_jobs(\n",
    "        self,\n",
    "        skills: Optional[List[str]] = None,\n",
    "        locations: Optional[List[str]] = None,\n",
    "        companies: Optional[List[str]] = None,\n",
    "        min_salary: Optional[float] = None,\n",
    "        work_mode: Optional[str] = None,\n",
    "        n_results: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Filter jobs using structured criteria.\"\"\"\n",
    "        return self.retriever.retrieve_by_filters(\n",
    "            skills, locations, companies, min_salary, work_mode, n_results\n",
    "        )\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# TESTING & EXAMPLES\n",
    "# ===================================================================\n",
    "\n",
    "def test_rag_system():\n",
    "    \"\"\"Test the RAG system with example queries.\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"üß™ TESTING RAG SYSTEM\")\n",
    "    logger.info(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    try:\n",
    "        rag = RAGSystem(\n",
    "            jobs_csv_path=\"prioritized_jobs.csv\",\n",
    "            rebuild_index=True  # Set to True first time\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize RAG system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test 1: Semantic search\n",
    "    logger.info(\"\\nüìù TEST 1: Semantic Search\")\n",
    "    results = rag.search(\n",
    "        query=\"Python developer with machine learning experience in Bangalore\",\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"\\nTop 5 Results:\")\n",
    "    for i, job in enumerate(results, 1):\n",
    "        logger.info(f\"\\n{i}. {job['position_title']} at {job['company_name']}\")\n",
    "        logger.info(f\"   Location: {job['location_city']}\")\n",
    "        logger.info(f\"   Skills: {str(job['skills_required'])[:60]}...\")\n",
    "        logger.info(f\"   Relevance: {job['relevance_score']:.1f}/100\")\n",
    "        logger.info(f\"   Priority: {job['final_priority_score']:.1f}/100\")\n",
    "    \n",
    "    # Test 2: Structured filtering\n",
    "    logger.info(\"\\n\\nüìù TEST 2: Structured Filtering\")\n",
    "    results = rag.filter_jobs(\n",
    "        skills=[\"python\", \"sql\"],\n",
    "        locations=[\"Bangalore\", \"Remote\"],\n",
    "        min_salary=500000,\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"\\nFiltered Results ({len(results)} jobs):\")\n",
    "    for i, job in enumerate(results, 1):\n",
    "        logger.info(f\"\\n{i}. {job['position_title']} at {job['company_name']}\")\n",
    "        logger.info(f\"   Salary: ‚Çπ{job['salary_max']/100000:.1f} LPA\")\n",
    "        logger.info(f\"   Priority: {job['final_priority_score']:.1f}/100\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run tests\n",
    "    test_rag_system()\n",
    "    \n",
    "    # Interactive search (optional)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç INTERACTIVE SEMANTIC SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Enter search queries (or 'quit' to exit)\\n\")\n",
    "    \n",
    "    try:\n",
    "        csv_path = r\"D:\\Projects By Month\\November 2025\\Placement Mail Analysis System\\.venv\\Phase_scripts\\Phase 4\\prioritized_jobs.csv\"\n",
    "        rag = RAGSystem(jobs_csv_path=csv_path)\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nüîé Search: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            results = rag.search(query, n_results=3)\n",
    "            \n",
    "            print(f\"\\nüìä Found {len(results)} results:\\n\")\n",
    "            for i, job in enumerate(results, 1):\n",
    "                print(f\"{i}. {job['position_title']} at {job['company_name']}\")\n",
    "                print(f\"   üìç {job['location_city']} | üí∞ ‚Çπ{job['salary_max']/100000:.1f} LPA\")\n",
    "                print(f\"   ‚≠ê Relevance: {job['relevance_score']:.1f}/100\\n\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Goodbye!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
