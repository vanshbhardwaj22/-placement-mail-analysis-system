{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd872103",
   "metadata": {},
   "source": [
    "### **Import Libraries and Global Dependies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import concurrent.futures\n",
    "import ssl\n",
    "import certifi\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import socket\n",
    "import httplib2\n",
    "from datetime import datetime\n",
    "from threading import Lock\n",
    "import base64\n",
    "\n",
    "# Google API modules\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Encryption for token security\n",
    "from cryptography.fernet import Fernet\n",
    "from typing import List, Dict, Set, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55077632",
   "metadata": {},
   "source": [
    "### **Configuration Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        CONFIG = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(\"FATAL ERROR: config.json file not found. Please create config.json in this directory.\")\n",
    "\n",
    "# Extract config variables\n",
    "SCOPES = CONFIG[\"api_scopes\"]\n",
    "OUTPUT_CSV = CONFIG[\"output_csv\"]\n",
    "TOKEN_FILE = CONFIG[\"token_file\"]\n",
    "CREDENTIALS_FILE = CONFIG[\"credentials_file\"]\n",
    "FILTERS = CONFIG[\"filters\"]\n",
    "\n",
    "# Logging configuration (shared across notebook)\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_file = os.path.join(log_dir, f\"retrieval_{timestamp}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Configuration and logging initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc58162",
   "metadata": {},
   "source": [
    "### **Gmail Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmailAuthError(Exception):\n",
    "    \"\"\"Raised when Gmail authentication or service creation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "def _load_encryption_key():\n",
    "    \"\"\"Load or create encryption key for token protection.\"\"\"\n",
    "    key_path = os.path.expanduser(\"~/.gmail_token_key\")\n",
    "    if not os.path.exists(key_path):\n",
    "        key = Fernet.generate_key()\n",
    "        with open(key_path, \"wb\") as f:\n",
    "            f.write(key)\n",
    "        os.chmod(key_path, 0o600)\n",
    "        logger.info(f\"ðŸ”‘ New encryption key created at {key_path}\")\n",
    "    else:\n",
    "        with open(key_path, \"rb\") as f:\n",
    "            key = f.read()\n",
    "    return Fernet(key)\n",
    "\n",
    "def get_gmail_service():\n",
    "    \"\"\"Authenticate user and return Gmail service object securely.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Initializing Gmail authentication process...\")\n",
    "        creds = None\n",
    "        fernet = _load_encryption_key()\n",
    "\n",
    "        if os.path.exists(TOKEN_FILE):\n",
    "            logger.info(f\"Loading encrypted token from {TOKEN_FILE}\")\n",
    "            try:\n",
    "                with open(TOKEN_FILE, \"rb\") as f:\n",
    "                    encrypted = f.read()\n",
    "                decrypted = fernet.decrypt(encrypted).decode()\n",
    "                creds = Credentials.from_authorized_user_info(json.loads(decrypted), SCOPES)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Token decryption failed: {e}. Proceeding with new authentication.\")\n",
    "                creds = None\n",
    "        else:\n",
    "            logger.warning(\"Token file not found â€” initiating new authentication flow.\")\n",
    "\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                try:\n",
    "                    logger.info(\"Refreshing expired Gmail token...\")\n",
    "                    creds.refresh(Request())\n",
    "                    logger.info(\"Token refreshed successfully.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Token refresh failed: {e}\")\n",
    "                    creds = None\n",
    "            else:\n",
    "                MAX_OAUTH_RETRIES = 3\n",
    "                for attempt in range(MAX_OAUTH_RETRIES):\n",
    "                    try:\n",
    "                        logger.info(f\"Starting OAuth flow (attempt {attempt+1}/{MAX_OAUTH_RETRIES})...\")\n",
    "                        flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_FILE, SCOPES)\n",
    "                        creds = flow.run_local_server(\n",
    "                            port=0,\n",
    "                            authorization_prompt_message=\"Please authorize this app to access Gmail.\",\n",
    "                            success_message=\"Authentication successful! You may close this tab.\",\n",
    "                            open_browser=True\n",
    "                        )\n",
    "                        logger.info(\"OAuth authentication completed successfully.\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"OAuth flow attempt {attempt+1} failed: {e}\")\n",
    "                        if attempt < MAX_OAUTH_RETRIES - 1:\n",
    "                            logger.info(\"Retrying in 10 seconds...\")\n",
    "                            time.sleep(10)\n",
    "                        else:\n",
    "                            raise GmailAuthError(\"OAuth flow failed after multiple retries.\") from e\n",
    "\n",
    "            if creds and creds.valid:\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(TOKEN_FILE) or \".\", exist_ok=True)\n",
    "                    encrypted = fernet.encrypt(creds.to_json().encode())\n",
    "                    with open(TOKEN_FILE, \"wb\") as token:\n",
    "                        token.write(encrypted)\n",
    "                    os.chmod(TOKEN_FILE, 0o600)\n",
    "                    logger.info(f\"Encrypted token saved to {TOKEN_FILE}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to save encrypted token: {e}\")\n",
    "\n",
    "        service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "        logger.info(\"Gmail service authenticated and ready.\")\n",
    "        return service\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Gmail authentication failed: {e}\")\n",
    "        raise GmailAuthError(\"Gmail service initialization failed.\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03a594",
   "metadata": {},
   "source": [
    "### **Relevance Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc850ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_email(sender: str, subject: str, snippet: str, body: str = \"\") -> bool:\n",
    "    \"\"\"Enhanced rule-based email relevance check with full body analysis.\"\"\"\n",
    "    if not sender:\n",
    "        return False\n",
    "\n",
    "    sender = sender.lower()\n",
    "    subject = (subject or \"\").lower()\n",
    "    snippet = (snippet or \"\").lower()\n",
    "    body = (body or \"\").lower()\n",
    "\n",
    "    allowed_domains = FILTERS[\"allowed_sender_domains\"]\n",
    "    spam_keywords = FILTERS[\"spam_subject_keywords\"]\n",
    "    placement_terms = FILTERS[\"required_placement_terms\"]\n",
    "\n",
    "    if not any(domain in sender for domain in allowed_domains):\n",
    "        return False\n",
    "\n",
    "    spam_text = f\"{subject} {snippet} {body}\"\n",
    "    if any(word in spam_text for word in spam_keywords):\n",
    "        return False\n",
    "\n",
    "    all_text = f\"{subject} {snippet} {body}\"\n",
    "    if not any(word in all_text for word in placement_terms):\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d034e",
   "metadata": {},
   "source": [
    "### **State Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_IDS_FILE = CONFIG.get(\"processed_ids_file\", \"state/processed_message_ids.txt\")\n",
    "ATTACHMENTS_DIR = CONFIG.get(\"attachments_dir\", \"attachments\")\n",
    "GMAIL_QUERY = CONFIG.get(\"gmail_query\", \"placement OR internship OR job OR hiring\")\n",
    "\n",
    "def load_processed_ids(path: str) -> set:\n",
    "    \"\"\"Load already processed Gmail message IDs from a text file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return {line.strip() for line in f if line.strip()}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load processed IDs from {path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def save_processed_ids(path: str, new_ids: set) -> None:\n",
    "    \"\"\"Append new processed IDs to existing set and persist to disk.\"\"\"\n",
    "    existing = load_processed_ids(path)\n",
    "    merged = existing.union(new_ids)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for mid in sorted(merged):\n",
    "                f.write(mid + \"\\n\")\n",
    "        logger.info(f\"Saved {len(merged)} total processed IDs to {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save processed IDs to {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a1efc",
   "metadata": {},
   "source": [
    "### **MIME Parsing Helper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_base64_urlsafe(data: str) -> str:\n",
    "    \"\"\"Decode base64url-encoded string to UTF-8 text.\"\"\"\n",
    "    if not data:\n",
    "        return \"\"\n",
    "    try:\n",
    "        padded_data = data + \"=\" * ((4 - len(data) % 4) % 4)\n",
    "        decoded_bytes = base64.urlsafe_b64decode(padded_data.encode(\"ASCII\"))\n",
    "        return decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_payload(payload: dict) -> str:\n",
    "    \"\"\"Recursively extract all text/plain and text/html content from Gmail payload.\"\"\"\n",
    "    texts = []\n",
    "\n",
    "    def walk(part: dict) -> None:\n",
    "        mime_type = part.get(\"mimeType\", \"\")\n",
    "        body = part.get(\"body\", {})\n",
    "        data = body.get(\"data\")\n",
    "        parts = part.get(\"parts\", [])\n",
    "\n",
    "        if data and mime_type.startswith(\"text/\"):\n",
    "            texts.append(_decode_base64_urlsafe(data))\n",
    "\n",
    "        for subpart in parts:\n",
    "            walk(subpart)\n",
    "\n",
    "    walk(payload)\n",
    "    return \"\\n\\n\".join(texts).strip()\n",
    "\n",
    "def collect_pdf_parts(payload: dict) -> list:\n",
    "    \"\"\"Find all PDF attachments in payload. Returns (filename, attachmentId) tuples.\"\"\"\n",
    "    pdfs = []\n",
    "\n",
    "    def walk(part: dict) -> None:\n",
    "        filename = part.get(\"filename\", \"\").strip()\n",
    "        body = part.get(\"body\", {})\n",
    "        attachment_id = body.get(\"attachmentId\")\n",
    "        parts = part.get(\"parts\", [])\n",
    "\n",
    "        if filename.lower().endswith(\".pdf\") and attachment_id:\n",
    "            pdfs.append((filename, attachment_id))\n",
    "\n",
    "        for subpart in parts:\n",
    "            walk(subpart)\n",
    "\n",
    "    walk(payload)\n",
    "    return pdfs\n",
    "\n",
    "def download_pdfs_for_message(service, user_id: str, message_id: str, payload: dict, attachments_dir: str) -> list:\n",
    "    \"\"\"Download all PDF attachments for a message. Returns saved file paths.\"\"\"\n",
    "    pdf_parts = collect_pdf_parts(payload)\n",
    "    if not pdf_parts:\n",
    "        return []\n",
    "\n",
    "    message_dir = os.path.join(attachments_dir, message_id)\n",
    "    os.makedirs(message_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "\n",
    "    for filename, attachment_id in pdf_parts:\n",
    "        try:\n",
    "            att = service.users().messages().attachments().get(\n",
    "                userId=user_id, messageId=message_id, id=attachment_id\n",
    "            ).execute()\n",
    "            \n",
    "            data = att.get(\"data\")\n",
    "            if not data:\n",
    "                continue\n",
    "\n",
    "            file_bytes = base64.urlsafe_b64decode(data + \"=\" * ((4 - len(data) % 4) % 4))\n",
    "            file_path = os.path.join(message_dir, filename)\n",
    "            \n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(file_bytes)\n",
    "            \n",
    "            saved_files.append(file_path)\n",
    "            logger.debug(f\"Saved PDF: {filename}\")\n",
    "            \n",
    "        except HttpError as e:\n",
    "            logger.warning(f\"Failed PDF {filename} for {message_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF download error {filename}: {e}\")\n",
    "\n",
    "    return saved_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b9c3a",
   "metadata": {},
   "source": [
    "### **Fetcher Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_emails():\n",
    "    \"\"\"Production Gmail email fetcher with full body extraction, PDF downloads, and duplicate prevention.\"\"\"\n",
    "    service = get_gmail_service()\n",
    "    processed_ids = load_processed_ids(PROCESSED_IDS_FILE)\n",
    "    newly_processed_ids = set()\n",
    "\n",
    "    # Phase 1: List message IDs\n",
    "    logger.info(f\"Fetching message IDs for query: {GMAIL_QUERY}\")\n",
    "    messages = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            results = service.users().messages().list(\n",
    "                userId=\"me\", q=GMAIL_QUERY, maxResults=500, pageToken=next_page_token\n",
    "            ).execute()\n",
    "        except HttpError as e:\n",
    "            logger.warning(f\"List error: {e}\")\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "        except ssl.SSLError as e:\n",
    "            logger.warning(f\"SSL error: {e}\")\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "\n",
    "        batch = results.get(\"messages\", [])\n",
    "        if not batch:\n",
    "            break\n",
    "        messages.extend(batch)\n",
    "        next_page_token = results.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Raw messages found: {len(messages)}\")\n",
    "\n",
    "    messages = [m for m in messages if m[\"id\"] not in processed_ids]\n",
    "    total_to_process = len(messages)\n",
    "    logger.info(f\"New messages to process: {total_to_process}\")\n",
    "\n",
    "    if total_to_process == 0:\n",
    "        logger.info(\"No new emails to process.\")\n",
    "        return\n",
    "\n",
    "    # Phase 2: Batch fetch\n",
    "    BATCH_SIZE = 15\n",
    "    MAX_RETRIES = 3\n",
    "    all_results = []\n",
    "    error_count = total_attempts = 0\n",
    "\n",
    "    def batch_callback(request_id: str, response: dict, exception):\n",
    "        nonlocal error_count, total_attempts, all_results, newly_processed_ids\n",
    "        \n",
    "        total_attempts += 1\n",
    "        if exception:\n",
    "            error_count += 1\n",
    "            logger.warning(f\"Callback error {request_id}: {exception}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            payload = response.get(\"payload\", {})\n",
    "            snippet = response.get(\"snippet\", \"\")\n",
    "            message_id = response.get(\"id\", request_id)\n",
    "\n",
    "            headers = payload.get(\"headers\", [])\n",
    "            subject = next((h[\"value\"] for h in headers if h[\"name\"].lower() == \"subject\"), \"No Subject\")\n",
    "            sender = next((h[\"value\"] for h in headers if h[\"name\"].lower() == \"from\"), \"Unknown\")\n",
    "            date = next((h[\"value\"] for h in headers if h[\"name\"].lower() == \"date\"), \"Unknown\")\n",
    "\n",
    "            body_text = extract_text_from_payload(payload)\n",
    "            pdf_files = download_pdfs_for_message(service, \"me\", message_id, payload, ATTACHMENTS_DIR)\n",
    "\n",
    "            if is_relevant_email(sender, subject, snippet, body_text):\n",
    "                all_results.append({\n",
    "                    \"MessageId\": message_id, \"Sender\": sender, \"Subject\": subject,\n",
    "                    \"Date\": date, \"Preview\": snippet[:200], \"Body\": body_text,\n",
    "                    \"PDFs\": \";\".join(pdf_files), \"WordCount\": len(body_text.split())\n",
    "                })\n",
    "                logger.info(f\"RELEVANT: {subject[:60]}...\")\n",
    "            else:\n",
    "                logger.debug(f\"ðŸ“Ž Skipped: {subject[:60]}...\")\n",
    "\n",
    "            newly_processed_ids.add(message_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Callback processing error {request_id}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    # Execute batches\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Processing {total_to_process} messages in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "    for i in range(0, total_to_process, BATCH_SIZE):\n",
    "        if error_count / max(total_attempts, 1) > 0.25:\n",
    "            logger.error(\"Aborting: Error rate >25%\")\n",
    "            break\n",
    "\n",
    "        batch_msgs = messages[i:i + BATCH_SIZE]\n",
    "        batch = service.new_batch_http_request(callback=batch_callback)\n",
    "\n",
    "        for msg in batch_msgs:\n",
    "            req = service.users().messages().get(userId=\"me\", id=msg[\"id\"], format=\"full\")\n",
    "            batch.add(req, request_id=msg[\"id\"])\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                batch.execute()\n",
    "                logger.info(f\"Batch {i//BATCH_SIZE + 1}/{total_to_process//BATCH_SIZE + 1}\")\n",
    "                time.sleep(2 + random.uniform(0.2, 0.8))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Batch {i//BATCH_SIZE + 1} attempt {attempt}: {e}\")\n",
    "                time.sleep(5 * attempt)\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    error_count += len(batch_msgs)\n",
    "\n",
    "    # Phase 3: Save results\n",
    "    if all_results:\n",
    "        mode = \"a\" if os.path.exists(OUTPUT_CSV) else \"w\"\n",
    "        fieldnames = [\"MessageId\", \"Sender\", \"Subject\", \"Date\", \"Preview\", \"Body\", \"PDFs\", \"WordCount\"]\n",
    "        \n",
    "        with open(OUTPUT_CSV, mode, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if mode == \"w\":\n",
    "                writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "        \n",
    "        logger.info(f\"Saved {len(all_results)} relevant emails to {OUTPUT_CSV}\")\n",
    "\n",
    "    if newly_processed_ids:\n",
    "        save_processed_ids(PROCESSED_IDS_FILE, newly_processed_ids)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"COMPLETE: {elapsed:.1f}s | {len(all_results)} relevant | {len(newly_processed_ids)} processed | {error_count}/{total_attempts} errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_emails()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Placement Mail Analysis System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
